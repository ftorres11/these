\begin{figure}[t]
	\resizebox{\textwidth}{!}{
	\begin{tikzpicture}[
		scale=.3,
		font={\footnotesize},
		node distance=.5,
		label distance=3pt,
		wide/.style={yscale=.75},
		net/.style={draw,trapezium,trapezium angle=75,inner sep=3pt},
		enc/.style={net,shape border rotate=270},
		dec/.style={net,shape border rotate=90},
		txt/.style={inner sep=3pt},
		loose/.style={looseness=.6},
		sg/.style={blue!60},
	]
	\matrix[
		tight,row sep=6,column sep=16,
		cells={scale=.3,},
	] {
		\&\&\&\&\&
		\node[dec] (guided-back) {guided \\ backprop}; \&
		\node[wide,label=90:$\ipder{_G L_C}{x}$] (guided) {\fig[.1]{grad/fig/method/guided_gradient}}; \\
		\node[label=90:input image $x$] (in) {\fig[.1]{grad/fig/method/input}}; \&
		\node[enc] (net) {network \\ $f$}; \&\&\&
		\node[box] (class) {classification \\ loss $L_C$}; \&
		\node[dec] (back) {standard \\ backprop}; \&
		\node[wide,label=90:$\ipder{L_C}{x}$] (grad) {\fig[.1]{grad/fig/method/gradient}}; \&
		\node[box] (reg) {regularization \\ loss $L_R$}; \\
		\&\&\&\&\&\&\&
		\node[op] (plus) {$+$}; \&
		\node[txt] (loss) {total loss \\ $L = L_C + $\\ $\lambda L_R$}; \\
	};

	\draw[->]
		(in) edge (net)
		(net) edge node[above] {logits $y$} (class)
		(class) edge (back)
		(back) edge (grad)
		(guided-back) edge[sg] (guided)
		(grad) edge (reg)
		(reg) edge (plus)
		(plus) edge (loss)
		(class) edge[sg,out=90,in=180] (guided-back)
		(class) edge[loose,out=-50,in=180] (plus)
		(guided) edge[sg,out=0,in=90] node[pos=.2,font=\large,label=0:stop gradient] {//} (reg)
		(net) edge[dashed,out=60,in=150] (guided-back)
		(net) edge[dashed,out=30,in=150] (back)
		;
	\end{tikzpicture}
	}
	\caption{\textbf{Interpretable gradient learning}. For an input image $x$, we obtain the logit 
	vector $y = f(x; \theta)$ by a forward pass through the network $f$ with parameters $\theta$. 
	We compute the classification loss $L_C$ by softmax and cross-entropy~\eq{class}, \eq{ce}. We 
	obtain the standard gradient $\ipder{L_C}{x}$ and guided gradient $\ipder{_G L_C}{x}$ by two 
	backward passes (dashed) and compute the regularization loss $L_R$ as the error between the 
	two~\eq{reg},\eq{mae}-\eq{cos}. The total loss is $L = L_C + \lambda L_R$~\eq{total}. Learning 
	is based on $\ipder{L}{\theta}$, which involves differentiation of the entire computational 
	graph except the guided backpropagation branch (blue).}
	\label{fig:grad_method}
\end{figure}