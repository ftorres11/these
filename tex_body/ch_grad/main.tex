\chapter{A learning paradigm for interpretable gradients}
\chaptertoc{}
\label{ch:grad}
%--------------------------------------------------------------------------------------------------
\section{Introduction}
\noindent A recurring issue faced by both neural networks and transformers is their inherent
lack of interpretability. These models are primarily optimized for high performance in their 
designated tasks. Yet, reflecting upon the information that can be drawn out of a 
model without too much an effort; we observe that the gradient of any given model although noisy,
contains relevant information regarding the behavior after a given input, after all because of this,
several interpretability methods are designed around it.\\

\noindent However,  the effective utilization of gradients in interpretability methods remains a 
pressing question. \textit{How can we leverage gradient better?}, previous interpretability 
approaches have relied on the stand alone gradient information such as Guided Backpropagation 
(\cite{guidedbackprop}) and Smoothgrad (\cite{smilkov2017smoothgrad}). On  another hand as 
seen in previous chapters some CAM variants are designed around it, like Grad-CAM 
(\cite{selvaraju2017grad}), Grad-CAM++ (\cite{chattopadhay2018grad}) and Axiom-CAM (\cite{axiombased}).
Nevertheless, it is worth reflecting that gradient plays a more prominent role during 
the training phase of a model, particularly as a cornerstone in this process, we can not help but
reflect upon \textit{how can we leverage upon the gradient for interpretability during training?}.\\

\noindent In this chapter, we propose a modification to the training process of deep models by 
introducing of a regularization term to the error function. This term is aimed at constraining 
the gradient by aligning it with guided back-propagation in the input space.


\input{tex_body/ch_grad/tex/def}
\input{tex_body/ch_grad/tex/expes}
\input{tex_body/ch_grad/tex/qual}
\newpage
\input{tex_body/ch_grad/tex/quant}
\newpage

