%--------------------------------------------------------------------------------------------------
\section{Discussion}
\label{sec:discuss_opticam}
\noindent Opti-CAM is constructed following the definition of \emph{CAM-based} saliency maps. In 
particular, according to \autoref{eq:sal}, we optimize the variable $w^c_k$ to construct a 
saliency map $S^c_\ell(\mathbf{u})$, maximizing the predicted probability of the explanation 
obtained by performing element-wise multiplication with an input image. \\

\paragraph{Classifier-Centric Explanations} As a consequence of optimizing prediction probability 
of explanation maps, our approach highlights the salient regions in the image. We observe 
in CNNs, salient information is spread across the input image and not often centered within the 
object of interest. These models learn biases within data, and use context to construct a 
prediction. Studies have demonstrated instances in fine-grained image classification, where the 
model learns the background of images instead of the object of interest \autocite{petryk2022guiding}.  
Classifier-centric explanations can demonstrate situations where this is the case. Moreover, 
classifier-centric explanations do not leave space for human interpretations about the inference 
process of a model. Thus removing one factor of bias towards interpretation.\\

\paragraph{Localization Properties} A direct consequence of generating classifier-centric 
explanations, is a trade-off in localization properties. Compared to current 
attribution methods, Opti-CAM fails in this regard. Preliminary studies suggesting the evaluation 
of saliency maps based on localization, can be traced back to the work by (\cite{shetty2019not}, 
\cite{zhou2018interpreting}, \cite{rao2022towards}). This failure however is desired. Furthermore, 
it highlights that context is important towards prediction of a model; and as such, localization 
is an ill-fitted requirement to assess interpretability.\\

\paragraph{Computational Complexity} Opti-CAM generates an optimized saliency map for every image. 
However, in comparison to current high-performing \emph{CAM} methods, the trade-off between 
complexity and performance favors our approach. In detail, methodologies such as \emph{Ablation-CAM} and 
\emph{Score-CAM} require as many forward passes as the number of channels in the target layer of 
interest. In contrast, our approach requires as much as a hundred optimization steps. 
Additionally, these optimization steps are not as complex as a complete forward pass through the 
network: our optimization objective requires forwarding the product of the optimization variable,
with the feature maps from our target layer until the classifier. Therefore, Opti-CAM requires
less memory resources; as well as being faster in running time.\\

\paragraph{Average Gain} Current classification/recognition metrics are not robust and complete,
to differentiate interpretability properties of different approaches. This is 
demonstrated in the work of \autocite{poppi2021revisiting} where Fake-CAM achieves almost perfect 
\gls{ad} but fails completely on \gls{ai}. Average Gain is designed to address this remark. In 
particular, Average Gain acts as the complement of \gls{ad}: we measure the positive impact that 
an explanation poses using Average Gain. Conversely, we observe that \gls{ai} is a metric that on 
itself does not answer to anything in particular: on a real world application we do not care in 
how many instances the explanation map is better than an input image; we focus on the 
effect an explanation has over the classifier. This efficiently covered with AD and AG\\