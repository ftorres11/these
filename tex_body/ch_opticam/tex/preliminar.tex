\section{Motivation}
\label{sec:oc_motiv}
From the \gls{cam} methods defined in \autoref{rel:sub_post}, we take particular interest on 
Score-CAM. In particular, Score-CAM considers each feature map as a mask in isolation. But, 
\emph{what about linear combinations?}
Given a vector $\vw \in \real^{K_\ell}$ with $w_k$ its $k$-th element, let
\begin{equation}
	F(\vw) \defn f \left( \mathbf{u} \odot n \left( \operatorname{up} \left(
		\displaystyle\sum_k w_k A^k_\ell
	\right) \right) \right)_c.
\label{eq:s-obj}
\end{equation}
If we assume that $\mathbf{u}_b = \vzero$ in~\eq{s-cam} and define $n(\vzero) \defn \vzero$ 
in~\eq{norm}, then we can rewrite the right-hand side of~\eq{s-cam} as
\begin{equation}	
	\frac{F(\vw_0 + \delta \ve_k) - F(\vw_0)}{\delta},
\label{eq:s-cam2}
\end{equation}
where $\vw_0 = \vzero$, $\delta = 1$ and $\ve_k$ is the $k$-th standard basis vector of 
$\real^{K_\ell}$. This resembles the numerical approximation of the derivative 
$\pder{F}{w_k}(\vw_0)$, except that $\delta$ is not small as usual. One could compute derivatives 
efficiently by standard backpropagation instead. It is then possible to iteratively optimize $F$ 
with respect to $\vw$, starting at any $\vw_0$.\\

\noindent As an alternative, consider masking-based methods relying on optimization in the input 
space, like \emph{meaningful perturbations} (MP) \parencite{fong2017interpretable} or \emph{extremal 
perturbations} \parencite{fong2019understanding}. In general, optimization takes the form
\begin{equation}
	S^c(\mathbf{u}) \defn \arg\max_{\vm \in \cM} f(\mathbf{u} \odot n(\operatorname{up}(\vm)))_c + \lambda R(\vm).
\label{eq:mask}
\end{equation}
Here, a mask $\vm$ is directly optimized and does not rely on feature maps, hence the saliency 
map $S^x(\mathbf{u})$ is not connected to any layer $\ell$. The mask is at the same or lower 
resolution than the input image. In the latter case, upsampling is still necessary.\\

\noindent In this approach, one indeed computes derivatives by backpropagation and iteratively 
optimizes $\vm$. However, because $\vm$ is high-dimensional, there are constraints expressed by 
$\vm \in \cM$, \eg $\vm$ has a certain norm, and regularizers like $R(\vm)$, \eg $\vm$ is smooth in 
a certain way. This makes optimization harder or more expensive and introduces more hyperparameters 
like $\lambda$. One could simply constrain $\vm$ to lie in the linear span of $\{A_\ell^k\}_{k=1}
^{K_\ell}$ instead, like all CAM-based methods.

