%--------------------------------------------------------------------------------------------------
\section{Experiments}
\label{sec:oc_exp}
We evaluate Opti-CAM and compare it quantitatively and qualitatively against other state-of-the-art 
methods on a number of datasets and networks. We report classification metrics with execution times 
and we provide visualizations, an ablation study and a study on the suitability of localization 
ground truth.
%A sanity check, additional classification results, localization metrics, more ablations, more 
%visualizations \redred{and code} are given in supplementary material}

\subsection{Implementation details}
\label{sec:oc_details}

All input images are resized to $224 \times 224 \times 3$. To optimize the saliency map with 
Opti-CAM~\eq{opt}, we use the Adam \autocite{kingma2014adam} optimizer with learning rate $0.1$ by 
default, setting the maximum number of iterations to $100$ and stopping early when the change in 
loss is less than $10^{-10}$. For VGG16, we generate the saliency map \eq{v-sal} from the feature 
maps of the last convolutional layer before max pooling by default, \ie convolutional layer 3 of 
block 5. For ResNet50, we choose the last convolutional layer by default, \ie convolutional layer 3 
of bottleneck 2 of block 4. For ViT and DeiT, we choose the last self-attention block by default, 
\ie layer normalization of self-attention block 12. Ablations concerning the layer $\ell$ and the 
convergence of Opti-CAM are included in \autoref{sec:oc_more-ablation}.

\subsection{Datasets}
\label{sec:oc_data}

\paragraph{ImageNet}
We use the validation set of ImageNet ILSVRC 2012 (\cite{krizhevsky2012imagenet}, \cite{ILSVRC15}), 
containing $50,000$ images evenly distributed over the $1,000$ categories. For the ablation study 
and for timing, we sample $1,000$ images from this set. Concerning the localization experiments, 
bounding boxes from the localization task of ILSVRC
\footnote{\url{https://www.image-net.org/challenges/LSVRC/2012/index.php}} are used on the same 
validation set.

\paragraph{Medical data}

We use two medical image datasets, namely \emph{Chest X-ray} \autocite{kermany2018labeled} and 
\emph{Kvasir} \autocite{pogorelov2017kvasir}. 
%Complete qualitative and quantitative results are given 
%in the supplementary. Here we only provide visualizations.

%--------------------------------------------------------------------------------------------------

\paragraph{Networks}
\label{sec:oc_setup}

For all datasets, we use the pretrained ResNet50 \autocite{he2016deep} and VGG16 
\autocite{simonyan2015deep} networks with batch normalization \autocite{ioffe2015batch} from the 
Pytorch model zoo\footnote{\url{https://pytorch.org/vision/0.8/models.html}}. For ImageNet, we 
further use the pretrained ViT-B (16-224) \autocite{dosovitskiy2020image} and DeiT-B (16-224) 
\autocite{pmlr-v139-touvron21a} from Pytorch image models (timm)\footnote{\url{
    https://github.com/rwightman/pytorch-image-models}}.
% pre-trained on ImageNet-21k and fine-turned on ImageNet-1k~\autocite{ILSVRC15}.
%Regarding medical datasets, we fine-tune the networks as discussed in the supplementary material, 
%where we also provide the setting details.


%--------------------------------------------------------------------------------------------------

\subsection{Evaluation}
\label{sec:eval}

\paragraph{Metrics}
We use \emph{\gls{ad}} and \emph{\gls{ai}} \autocite{chattopadhay2018grad} metrics, as well 
as the proposed \emph{\gls{ag}}, to measure the effect on classification performance of masking 
the input image by a saliency map. In addition, we report \emph{ins} and \emph{\gls{del}}  
\autocite{petsiuk2018rise} and highlight their limitations. Using classification metrics, we show 
the limitations of using the localization ground truth for the evaluation of attribution methods. 
In \autoref{sec:oc_wsol}, we provide a number of localization metrics from the 
\emph{weakly-supervised object localization} (WSOL) task of ILSVRC2014
\footnote{\url{https://www.image-net.org/challenges/LSVRC/2014/index\#}}.

\paragraph{Methods}

We compare against the following state-of-the-art methods: Grad-CAM \autocite{selvaraju2017grad}, 
Grad-CAM++\cite{chattopadhay2018grad}, Score-CAM \autocite{wang2020score}, Ablation-CAM 
\autocite{ramaswamy2020ablation}, XGrad-CAM \autocite{axiombased}, Layer-CAM 
\autocite{jiang2021layercam}, ExtremalPerturbation \autocite{fong2019understanding} 
and HiRes-CAM \autocite{draelos2020use}. Implementations are obtained from the PyTorch CAM 
library\footnote{\url{https://github.com/jacobgil/pytorch-grad-cam}} or 
TorchRay\footnote{\url{https://github.com/facebookresearch/TorchRay}}. For transformer models, 
we also compare against raw attention \autocite{dosovitskiy2020image}, 
rollout \autocite{abnar2020quantifying} and TIBAV \cite{chefer2021transformer}\footnote{\url{
https://github.com/hila-chefer/Transformer-Explainability}}.

\paragraph{Image normalization}

It is standard that images are normalized before feeding them to a network. By doing so however, 
we cannot reproduce the results published for the baseline methods; rather, all results are 
improved dramatically. We can obtain results similar to published ones by \emph{not} normalizing. 
We believe normalization is important and we include it in all our experiments. 
In \ref{secsupl}, we provide more details and results without normalization.
