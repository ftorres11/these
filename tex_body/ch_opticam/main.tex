\chapter{Opti-CAM: Optimizing saliency maps for interpretability}
\chaptertoc{}
%--------------------------------------------------------------------------------------------------
\label{ch:opticam}
% Introduction, to move to a more verbatum approach

\noindent Within existing attribution approaches for interpretable saliency map generation, the CAM 
(\cite{zhou2016learning}) based family of methods takes special interest given its reliance of
existing  information and properties of a given model to generate explanations. In particular we 
note that following Equation \ref{eq:sal}, should the computation of the weighting coefficient 
$w_k^c$ be modified a different attribution is generated. Moreover, this computation can be altered
 for instance by relying on information found while performing the backward pass 
 (\cite{selvaraju2017grad}, \cite{chattopadhay2018grad}, \cite{axiombased}, 
 \cite{smilkov2017smoothgrad}) and the forward pass (\cite{wang2020score}) of the model during 
 inference.\\

\noindent Additionally, following attribution based methods such as extremal perturbations 
(\cite{fong2019understanding}) or IBA (\cite{schulz2020restricting}), 
we note that their class scores are optimized via gradient descent; in this regard, it can 
be stated that these masks then become variables within input-feature space, and the aforementioned 
scores then become a function of said masking. However it is important to point that optimizing 
these masks ultimately ends turning into an expensive process as several constraints are needed 
to be taken into consideration to further control the masking area.\\

\noindent Complementary to the computation of the weighting coefficient for the computation of 
saliency maps, we note that existing metrics to evaluate the interpretable properties of these 
attributions do provide complete insight onto how good an approach is. In particular, we take 
interest in Fake-CAM (\cite{poppi2021revisiting}) as this methodology raises questions with 
relation to AD \ref{eq:ad} and AI \ref{eq:ai}; as perfect scores are achieved by generated a 
saliency map where all the elements of the image have maximum importance bar one pixel.\\

\noindent With the observations previously mentioned, in this chapter we propose a CAM variant that 
generates saliency maps by optimizing the weighting coefficient $w_k^c$, while also introducing a 
novel metric to complement the existing evaluation of attribution methods.\\

\input{tex_body/ch_opticam/tex/preliminar}
\input{tex_body/ch_opticam/tex/def}
\input{tex_body/ch_opticam/tex/av_gain}
Opti-CAM is evaluated quantitatively using classification metrics and qualitatively by visualizing
 saliency maps.
\input{tex_body/ch_opticam/tex/qual}
\input{tex_body/ch_opticam/tex/quant_class}
\input{tex_body/ch_opticam/tex/quant_loc}






%In this work, we are interested in the interpretability of deep neural networks through the 
%generation of \emph{saliency maps}, highlighting regions of an image that are responsible for the 
%prediction. This originates in \emph{gradient-based} methods \citep{simonyan2013deep, 
%yosinski2015understanding}, including variants of backpropagation \citep{zeiler2014visualizing, 
%springenberg2014striving, bach2015pixel}. CAM~\citep{zhou2016learning} introduced class-specific 
%linear combinations of feature maps, and led to several alternative weighting schemes 
%\citep{ramaswamy2020ablation, wang2020score, muhammad2020eigen}, including the use of gradients 
%\citep{selvaraju2017grad, chattopadhay2018grad}. On the other hand, \emph{occlusion-} or 
%\emph{masking-based} methods \citep{dabkowski2017real, fong2017interpretable, fong2019understanding, 
%schulz2020restricting} remove regions in the image space while improving classification performance.

%Score-CAM~\cite{wang2020score} uses each feature map as a mask and defines a corresponding weight 
%based on the resulting increase of class score; hence, it is both CAM-based and masking-based but does
% not use gradients. It resembles the numerical gradient approximation, in that it needs \emph{one 
% forward pass per weight}. Instead, the analytical approach would be to use a linear combination of feature 
% maps as a mask, express the class score as a function of the weights and measure the gradient analytically,
%  in a \emph{single backward pass}. Then, \emph{why not use gradient descent to maximize the class score?} 
%  The optimal mask should highlight regions for which the network is most confident.

%\emph{Masking-based} methods, such as extremal perturbations~\citep{fong2019understanding} or 
%IBA~\citep{schulz2020restricting}, do use gradient descent to maximize the class score. The mask is now 
%a variable in the input or feature space and the class score is expressed as a function of the mask 
%directly. Because the variable being optimized is a high-dimensional image or tensor, additional constraints 
%or regularizers are needed to control \eg the smoothness and the salient area. This translates to 
%more hyperparameters or more expensive optimization.

%Motivated by the above, we introduce Opti-CAM, illustrated in \autoref{fig:idea}. We form a linear 
%combination of feature maps, where the weights are a variable. Treating it as a saliency map, we 
%form a masked version of the input image that is fed again to the network. Then, the logit of a given 
%class for the masked version of the input is maximized to obtain the optimal weights. Thus, Opti-CAM can 
%be seen as an analytical counterpart of Score-CAM that is optimized iteratively, or as a masking-based 
%method where the mask to be optimized lies in the linear span of the feature maps, like CAM-based methods.