\chapter{Opti-CAM: Optimizing saliency maps for interpretability}
\chaptertoc{}
%--------------------------------------------------------------------------------------------------
\label{ch:opticam}
% Introduction, to move to a more verbatum approach
\section{Introduction}
\label{sec:intro}
\noindent Within existing attribution approaches for interpretable saliency map generation, the CAM 
\autocite{zhou2016learning} based family of methods takes special interest given its reliance of
existing  information and properties of a given model to generate explanations. In particular, we 
note that following \autoref{eq:sal}, modifying computation of the weighting coefficient 
$w_k^c$ results in a different attribution being generated. Moreover, this computation can be altered,
 for instance by relying on information found while performing the backward pass 
 (\cite{selvaraju2017grad}, \cite{chattopadhay2018grad}, \cite{axiombased}, 
 \cite{smilkov2017smoothgrad}) and the forward pass \autocite{wang2020score} of the model during 
 inference. Nevertheless, we observe that among existing weighting coefficient computation 
 proposals, none has been directed at maximizing the predicted probability of the generated 
 saliency maps.

Complementary to CAM methods, we observe that within attribution methods based on extremal 
perturbations \autocite{fong2019understanding} or IBA \autocite{schulz2020restricting}, 
their class scores are optimized via gradient descent. In this regard, it can 
be stated that these masks then become variables within input-feature space, and the aforementioned 
scores then become a function of said masking. However it is important to point that optimizing 
these masks ultimately becomes an expensive process, as several constraints are needed 
to be taken into consideration to further control the masking area.

Drawing inspiration from the aforementioned observations, we set ourselves to design \emph{Opti-
CAM}, an attribution method that generates saliency maps with enhanced interpretability. In 
particular, we hypothesize that the weighting coefficient $w_k^c$ can be optimized to attain this 
task; moreover, we also suggest that should the predicted probability of the attribution map is 
optimized, we can gain insight within the regions of the image that appear to be the most important 
for the classifier. For this, we set up the preliminaries in \autoref{sec:oc_prelim} to fully 
define our approach in \autoref{sec:oc_def}\\

\noindent In addition to the proposal of an attribution method in this chapter, we aim towards the 
design of a complementary interpretability evaluation metric of saliency maps. In particular, 
based on the remarks found in Fake-CAM \autocite{poppi2021revisiting}; where it is noted that 
existing metrics such as \gls{ad} (\ref{eq:ad}) and \gls{ai} (\ref{eq:ai}) can be manipulated. As a 
result of this, we argue that a complementary criterion is missing regarding \textit{Objective 
Evaluation for Object Recognition}. In \autoref{sec:av_gain} we define this novel measurement 
under the name \gls{ag}. To support our approach, we demonstrate our generated saliency maps in 
\autoref{sec:oc_qual} and we evaluate them in \autoref{sec:oc_quant} and \autoref{sec:oc_loc}.

\noindent To sum up, with the observations previously mentioned, in this chapter we propose a CAM 
variant that generates saliency maps by optimizing the weighting coefficient $w_k^c$, while also 
introducing a novel metric to complement the existing evaluation of attribution methods.\\

\input{tex_body/ch_opticam/tex/preliminar}
\newpage
\input{tex_body/ch_opticam/tex/def}
\newpage
\input{tex_body/ch_opticam/tex/av_gain}
\newpage
Opti-CAM is evaluated quantitatively using classification metrics and qualitatively by visualizing
 saliency maps.
\input{tex_body/ch_opticam/tex/qual}
\newpage
\input{tex_body/ch_opticam/tex/quant_class}
\newpage
\input{tex_body/ch_opticam/tex/quant_loc}






%In this work, we are interested in the interpretability of deep neural networks through the 
%generation of \emph{saliency maps}, highlighting regions of an image that are responsible for the 
%prediction. This originates in \emph{gradient-based} methods \citep{simonyan2013deep, 
%yosinski2015understanding}, including variants of backpropagation \citep{zeiler2014visualizing, 
%springenberg2014striving, bach2015pixel}. CAM~\citep{zhou2016learning} introduced class-specific 
%linear combinations of feature maps, and led to several alternative weighting schemes 
%\citep{ramaswamy2020ablation, wang2020score, muhammad2020eigen}, including the use of gradients 
%\citep{selvaraju2017grad, chattopadhay2018grad}. On the other hand, \emph{occlusion-} or 
%\emph{masking-based} methods \citep{dabkowski2017real, fong2017interpretable, fong2019understanding, 
%schulz2020restricting} remove regions in the image space while improving classification performance.

%Score-CAM~\cite{wang2020score} uses each feature map as a mask and defines a corresponding weight 
%based on the resulting increase of class score; hence, it is both CAM-based and masking-based but does
% not use gradients. It resembles the numerical gradient approximation, in that it needs \emph{one 
% forward pass per weight}. Instead, the analytical approach would be to use a linear combination of feature 
% maps as a mask, express the class score as a function of the weights and measure the gradient analytically,
%  in a \emph{single backward pass}. Then, \emph{why not use gradient descent to maximize the class score?} 
%  The optimal mask should highlight regions for which the network is most confident.

%\emph{Masking-based} methods, such as extremal perturbations~\citep{fong2019understanding} or 
%IBA~\citep{schulz2020restricting}, do use gradient descent to maximize the class score. The mask is now 
%a variable in the input or feature space and the class score is expressed as a function of the mask 
%directly. Because the variable being optimized is a high-dimensional image or tensor, additional constraints 
%or regularizers are needed to control \eg the smoothness and the salient area. This translates to 
%more hyperparameters or more expensive optimization.

%Motivated by the above, we introduce Opti-CAM, illustrated in \autoref{fig:idea}. We form a linear 
%combination of feature maps, where the weights are a variable. Treating it as a saliency map, we 
%form a masked version of the input image that is fed again to the network. Then, the logit of a given 
%class for the masked version of the input is maximized to obtain the optimal weights. Thus, Opti-CAM can 
%be seen as an analytical counterpart of Score-CAM that is optimized iteratively, or as a masking-based 
%method where the mask to be optimized lies in the linear span of the feature maps, like CAM-based methods.