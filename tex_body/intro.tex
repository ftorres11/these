%--------------------------------------------------------------------------------------------------
\addchap{Introduction}
%\addcontentsline{toc}{chapter}{Introduction}
Human curiosity has led to the desire of understanding the world we inhabit, prompting us to seek
explanations for the phenomena we encounter. We derive this from the information we gather through 
sensory processing. Since most of the sensory information humans process pertains 
to vision, it could be argued that we live within a visual world. Conversely, this human curiosity 
has led to the development of technologies that have fundamentally altered the world: we have 
drastically changed our surroundings by building and adapting them to our needs. Moreover, societal 
development has been closely intertwined with technology: on one hand, the first human settlements 
date back to the surge of agriculture; while on the other hand, the industrial revolution started 
paving the world towards the modern era.\\

\paragraph{Computation and Computer Vision} Currently, one technology that has taken prominence is 
computation, as it affects our lives directly and indirectly. This can be seen in our reliance on 
devices such as computers and  cellphones. These products are the result of scientific breakthroughs 
and innovation. Nevertheless, to do science we need to process information, for which we have 
developed disciplines like mathematics and physics, which in turn can be aided with computation. 
Conversely, innovation within the last century has propelled computation further with the emergence 
of electronic computers. Aided by improvements in transistors and the rapid development of 
microprocessors, computers have become faster, smaller and more accessible; allowing for their 
adoption within society. In recent years, this technology has undergone a revolution with the surge 
and popularization of \emph{Artificial Intelligence} (AI), a promising field with countless 
possibilities for changing and improving human lives.\\

\noindent Artificial Intelligence refers to a discipline in computer science, aimed at developing 
systems capable of performing tasks, usually achieved with human intelligence. For example, AI 
systems learn from data: they recognize patterns and make decisions based on the data itself. 
Moreover, AI benefits from techniques such as Machine Learning, Deep Learning, Natural Language 
Processing and Computer Vision. However, \emph{what do we mean when we say a system learns?} In 
techniques such as Machine Learning, the goal is to develop models that given a certain collection 
of data, answer a specific task. Consequently, these models learn by updating their parameters 
based on the hidden structure of data and its statistics.\\

\noindent One particular field where Artificial Intelligence displays promise is \emph{Computer 
Vision}. Computer vision aims to replicate human vision capabilities with a machine. This endeavor 
can be understood alongside three axes: \emph{Recognition}, \emph{Reorganization} and 
\emph{Reconstruction} \autocite{malik2016three}; the three fundamental tasks of this discipline.
Through recognition, we identify and assign semantic values to elements in our environment. 
Conversely, on regrouping we organize elements in space, according to their characteristics or 
concepts. Finally, through reconstruction we identify elements in a scene, producing 
a model of the external world. In computer vision, we employ models to approximate these axes. 
Moreover, with the adoption of AI in computer vision, the capabilities of these models to emulate 
human vision have increased drastically, leading to their adoption in tasks such as recognition of 
individuals, processing of mail and medical diagnosis. In this thesis we  take special interest in 
the task of image recognition. On one hand, it is the dimension that is the easiest to understand. 
On another hand, given its simplicity, it is used to prototype and produce methodologies 
intersecting the complementary dimensions of computer vision study.\\

\noindent Computer Vision is one major field that modern AI has impacted greatly, shown by the 
progress it has seen in the last decade. In particular, with Convolutional Neural Networks (CNNs), a 
breakthrough on image recognition occurred. Efficient computation of this 
operation enabled its usage on larger collections of data, allowing computer vision models to 
improve their capabilities. Furthermore, these models have benefitted from constant development, in 
turn allowing to perform more complex tasks over time. On one hand, this has led to the creation 
of technologies robust enough to build autonomous vehicles. On the other hand, complex tasks such  
as medical diagnosis are receiving AI tools to facilitate them. More recently, another 
breakthrough has taken place with the introduction of the transformer architecture. 
In particular, this architecture allows for a high degree of abstraction, successively avoiding 
issues that convolutions face, such as inductive bias and difficulties towards generalization. As a 
result of this, transformers have overcome convolutions in terms of performance but complexity as 
well. Consequently, its no longer so much a question whether \textit{can a model achieve a given 
task?}, but rather a question on \textit{how can this model perform this task?}. The main 
issue regarding these questions lies within the size and complexity of deep models, where 
providing interpretable explanations has lead to the surge of a novel field of research 
(\cite{li2018deep}, \cite{guidotti2018survey}, \cite{bodria2021benchmarking}), interpretability 
and explainable AI.\\

\paragraph{Explainable AI} Following the permeation of intelligent vision systems into society and 
their direct impact into human lives; understanding their inner-working and limitations has become 
critical. In particular, since their complexity has increased alongside their performance, we are 
interested in unfolding this property in order to answer questions regarding their outputs; 
specially when failure cases can negatively affect a life. For instance, considering the medical 
practice and the involvement of AI in automated diagnosis, a misdiagnosis of a pathology can 
potentially derive in an incorrect treatment and loss of life. This is originally referred to, as the 
\emph{Desiredata of Interpretability Research}, first proposed in \emph{The Mythos of Model 
Interpretability} \autocite{mythos_interp}. Furthermore, in this work Lipton establishes the 
different properties a model should present in order to be considered interpretable: Transparency 
and Post-Hoc Interpretability. Current Interpretability research is grouped alongside these 
two properties.\\

\noindent Lipton suggests that a transparent model is one that can be summarized or explained 
in its entirely using few words or operations. However, due to the complexity displayed by current 
computer vision models, providing such interpretations for a model is challenging; in particular, 
most AI models nowadays contain parameters often counted in millions if not 
billions. Additionally, the computation of the sequence of operations requiring these parameters 
to produce an output is also complex in the sense that a forward pass often requires $10^9$ 
operations \autocite{openai_compute}. On a more active manner, \emph{Transparency} can be attained 
by the introduction of modifications to a model or its training procedure 
\autocite{zhang2021survey}. Several works achieve this with the introduction of small decision 
trees to summarize the forward pass of a model; as well as with the addition of regularization terms 
during training encouraging elements of the model to represent semantic concepts 
(\cite{bau2017network}, \cite{wu2018beyond}). We expand upon this on \autoref{rel:sub_transp}\\

%Moreover, achieving this kind of interpretability is 
%done via the introduction of small modifications on top of the model or its training process. 
%Simplify, concrete example or abstract way. SIMPLIFY.

\noindent Regarding post-hoc interpretability, Lipton suggests to leverage upon the complex 
structure of models and consequently provide explanations utilizing the already existing parameters 
within the network. This approach in turn allows for a large variance in methodologies since 
information can be extracted in a plethora of different manners in current CNNs and transformers. 
Moreover, this variance of explanations can be observed in the nature of the explanation itself: 
post-hoc interpretations are often presented via text as captions, and in images often using 
saliency maps, to name a few (\cite{bach2015pixel}, \cite{ribeiro2016should} 
\cite{zhou2016learning}). On \autoref{rel:sub_post} we explore these methods in more detail. 
Nevertheless, since explanations are computed to highlight relevant information describing the 
inference process of a model, they are not aligned to what a human would consider following the 
same questioning. For instance, an individual might identify the whiskers and ears of a cat as its 
defining characteristic; but a model can conversely highlight the snoot or eyes instead. On top of 
this, post-hoc interpretations can be obtained for any class a model consider: be it the correct 
one pertaining to an object of interest or one completely unrelated. Still, on practice researchers 
tend to focus on the first case mentioned, while instances where a model fails to provide a correct 
prediction should the ones where interest should be focused on.\\


\paragraph{Thesis objectives} This PhD thesis aims at studying image recognition models and 
building upon them to propose novel model interpretability approaches. In particular, we aim at 
improving both recognition and interpretability capabilities of model predictions. While existing 
approaches may present these properties, some limitations still remain: a high 
computational cost, a lack of consensus regarding evaluation procedures, and a disconnect between 
human interpretability and model interpretability.\\

\noindent Regarding the aforementioned computational cost, we can approximate this issue following 
Lipton's interpretable properties. On one hand, \emph{transparency} approaches often require 
training of additions to the network or of the network itself. We observe that this computational 
cost comes from these alterations: in order to explain the model, its performance should 
be maintained or not worsened. As a result of this, the model parameters are taken into 
consideration during this phase, although they are not modified. Moreover, an optimized training 
procedure can be introduced to achieve this; as well as the modifications themselves can be simple 
but meaningful. On the other hand, complexity on \emph{post-hoc interpretability} approaches 
presents some variance. In particular, since they are built on the already existing parameters 
and computational graph of a model; their combination building up to an approach produces this 
variability. Moreover, we highlight that simple approaches often require direct computations within 
the model: a forward pass and a backward pass. Nevertheless, more complex approaches often involve 
several forward passes in order to generate a representation. In this aspect, we argue that these 
complex methodologies could be further simplified or made sparse in order to reduce their cost. 
To answer this requirement, in Chapter \ref{ch:grad} and Chapter \ref{ch:castream} we propose 
methodologies requiring one training procedure per dataset, and presenting lightweight inferences. 
Additionally, on Chapter \ref{ch:opticam}, we propose a saliency map method that although a bit 
more expensive than traditional approaches, displays state-of-the-art interpretability properties.\\

%% Break into two paragraphs GENERAL REVISION.
\noindent On the topic of \emph{consensus of evaluation}, it has been observed that with the 
release of novel interpretability approaches, comparisons are not consistent between articles. For 
instance, we observe inconsistencies in the measurements of Grad-CAM++ 
\autocite{chattopadhay2018grad} when compared to its values reported on the article of Score-CAM 
\autocite{wang2020score}. However, this is not the only case as this is repeated alongside multiple 
studies (\cite{lee2021lfi},\cite{wang2020ss}, \cite{naidu2020cam}). Because of this, a direct 
comparison across several methodologies does not exist. Therefore, there is no clarity 
regarding the true performance of approaches. To address this, a benchmark using a standardized 
evaluation procedure should be performed to clear these shortcomings. In this thesis we conduct 
standardized experimentation, experiments with equal evaluation objectives follow the same 
procedure as observed in Chapter \ref{ch:opticam} and Chapter \ref{ch:castream}\\

\noindent Furthermore, explainable methods do not suffer only from lack of consensus of 
standardization. In particular different families of approaches evaluate different objectives. On 
one hand, some post-hoc interpretability methods measure the effects of prediction probability, 
by considering the product of a saliency map with an input image. On the other hand, 
other methodologies assess interpretability by measuring the accuracy of the explanations they 
provide instead. In this aspect, we argue that a standardized procedure should present have 
clearly defined objectives: measure the impact of explanations in prediction probability, and 
assess their recognition properties. We address this challenge setting our experimentation 
objectives clear: in Chapter \ref{ch:opticam} we evaluate recognition and localization properties 
of Opti-CAM, in Chapter \ref{ch:castream} our Cross Attention Stream is subjected interpretable to 
object recognition evaluation, as in Chapter \ref{ch:grad} as well.\\

\noindent Finally, on \emph{alignment between human and model interpretations}, we observe that 
based on the learning procedures both actors perceive, interpretations differ. On one hand, human 
learning is not standard; associations of concepts may differ according to societal factors, as well 
as biochemical ones: important factors describing a prediction differ between individuals. On the 
other hand, although semantic concepts can be similarly highlighted by different models; these 
attributions share similarities in how they  are addressed to what the model deems important. 
Taking into consideration this remark, we argue that human-centric interpretability approaches 
should be made explicitly different to model-centric ones. On top of this, model interpretability 
claims should be sustained with quantitative evaluation procedures. In this work, we present 
interpretability proposals aligned to model interpretability, and we validate each of our claims 
using quantitative methods.

\paragraph{Dissertation Outline}
%\addcontentsline{toc}{section}{Dissertation Outline}
\noindent This dissertation is aimed towards the development of interpretable image recognition 
models and is organized in the following manner: In Chapter \ref{ch:rel} we 
introduce a background for image recognition models (Section \ref{rel:sec_imrecon}) and the ensuing 
approaches developed to study the interpretability on them (Section \ref{rel:sec_int}). 
Additionally, we introduce evaluation procedures for these approaches which will be further used to 
evaluate our proposals.\\

\noindent In Chapter \ref{ch:opticam}, we propose Opti-CAM as a methodology that generates 
optimized saliency maps highlighting the relevant regions on an image towards image classification. 
In Section \ref{sec:av_gain} we extend existing evaluation metrics with a novel measurement for 
model confidence. On Sections \ref{sec:oc_qual} and \ref{sec:oc_quant} we evaluate the effect of 
these contributions towards interpretability assessment. Opti-CAM overall presents an approach that 
highlights saliency relating to the classifier, thus the saliency map generated performs the best 
in terms of interpretability metrics although is not highly aligned to human interpretations. 
On top of this, our novel metric \emph{Average Gain} complements current interpretability evaluation 
metrics, quantifying benefits in prediction confidence using saliency maps. We follow this 
procedure to further evaluate our proposals.\\


\noindent Chapter \ref{ch:castream} introduces the Cross Attention Stream, an approach that boosts 
existing architectures interpretable properties. We set up the modulus of this approach in 
Section \ref{sec:ca_defn} alongside its deployment on Section \ref{sec:ca_design}. 
In Sections \ref{sec:ca_qual} and \ref{sec:ca_quant} we demonstrate the benefits of using this
proposal. In particular, our CA stream is a transparency approach that is evaluated through 
post-hoc interpretability evaluation methods. Moreover, our approach learns an abstract 
representation of the predicted class by the model; enhancing prediction probability, and improving 
interpretability properties.\\

\noindent Chapter \ref{ch:grad} characterizes a gradient denoising approach with a gradient denoising 
methodology as an approach to enhance the training procedure of current models while improving 
interpretability properties. In Section \ref{sec:grad_defn}, we define the gradient denoising 
protocol alongside the regularization proposals to do so.
Section \ref{sec:grad_expes} illustrates the effects of this paradigm
in the trained models and its effects on interpretability. This approach provides a preliminary 
study on the introduction of a regularization term during model training to improve upon both 
recognition and interpretability properties. Furthermore, this approach is mostly exploratory and 
still requires further developments to be consequently employed in larger collections of data.\\
    
\noindent Finally, we draw conclusions on our work and detail future research perspectives.