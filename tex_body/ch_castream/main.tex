\chapter{CA-Stream: Attention-based pooling for interpretable image recognition}
\chaptertoc{}
\section{Introduction}
\label{sec:intro}
%--------------------------------------------------------------------------------------------------
\noindent Another way to approach interpretability can be pointed towards the current advances in recognition 
in general. In particular, with the introduction of the transformer architecture (\cite{vaswani2017attention})
 a switch in the paradigmn occurred where the best performing architectures contain the self-attention 
module as a building block. Moreover with the proposal of the Vision Transformer 
(\cite{dosovitskiy2020image}) transformers were adopted into computer vision, this module gained 
prominence as it allowed models to push the boundaries in existing benchmarks. This led to an 
expansion with models such as Swin-T (\cite{liu2021swin}), LeViT \cite{graham2021levit}. Conversely
hybrid architectures combining ideas from both CNN and transformers can be observed 
like Conformer (\cite{peng2021conformer}), Patchconvnet (\cite{touvron2021augmenting}),
while on another hand a modernization of CNNs in the shape of ConvNeXt (\cite{liu2022convnet}) drew 
inspiration from these models, whilst adressing their shortcomings in downstream tasks.\\

\noindent Although these models have pushed visual recognition to new frontiers, their interpretable
 properties still require further exploration, as in one hand conventional interpretability 
methodologies for CNNs do not translate properly into their domain, all the while the explanations 
obtained from these methods (\cite{abnar2020quantifying}) do not appear to have a proper evaluation 
protocol, resulting in research aimed at improved visualizations and their asssesment 
(\cite{chefer2021transformer}).

\noindent In this chapter we study the correlation between CAM and one such attention visualization 
proposal that is the raw attention found in the classification (\cls) token (\cite{devlin2018bert}).
In particular, we note that self attention is defined for all patch tokens including \cls, however 
we can generate a cross attention between this token and the feature maps found at any given depth 
of a CNN; this being expressed in via linear combination of feature maps with this token, ultimately 
resembling a class agnostic CAM. As an extension of this, we propose the inclusion of a 
cross-attention module used to train this token as a replacement of GAP (\cite{lin2013network}), 
onto already trained models boosting  both their recognition and interpretable properties.



%quite in great value due to their class agnostic behavior due their class token (\Th{[CLS]}) used for 
%classification(\cite{devlin2018bert}).

%Convolutional neural networks have had tremendous success in computer vision \cite{he2016deep,liu2022convnet} 
%but, due to their complexity, it is still an open problem how to explain or interpret their predictions. 
%The most concrete achievement in this direction is to localize in an image what regions a prediction can be 
%attributed to, by means of \emph{saliency maps}. \emph{Post-hoc} interpretability methods do so without changing 
%the network architecture or training process and \emph{class activation mapping} (CAM)~\cite{zhou2016learning} 
%has been a milestone in their development. CAM-based saliency maps are expressed as a linear combination of 
%feature maps followed by an activation function and it is the definition of weights that determines different 
%methods \cite{DBLP:journals/corr/SelvarajuDVCPB16,DBLP:journals/corr/abs-1710-11063,DBLP:journals/corr/abs-1910-01279}.
%Grad-CAM~\cite{DBLP:journals/corr/SelvarajuDVCPB16}, Grad-CAM++~\cite{DBLP:journals/corr/abs-1710-11063} and ScoreCAM~\cite{DBLP:journals/corr/abs-1910-01279}


%Vision transformers~\cite{dosovitskiy2020image} are now strong competitors of convolutional networks, 
%characterized by global interactions between patch embeddings in the form of \emph{self attention}. 
%Based on a classification (\cls) token, their pooling mechanism allows localization by means of 
%\emph{raw attention} maps. However, these maps are class agnostic, they can be of low quality~\cite{dino} 
%and dedicated interpretability methods are required~\cite{chefer2021transformer}.

%In this work, we make an important connection between CAM and the raw attention map of the \cls token. 
%In particular, self attention is defined on all patch tokens, including \cls. Focusing on the cross attention 
%between \cls and patch token embeddings, this is expressed as a collection of dot product similarities 
%between embeddings, followed by softmax. 
%We show that this collection of similarities is in fact a linear combination of feature maps, where the 
%weights are the elements of the \cls token embedding. Hence, 
%\textbf{the raw attention map of the \cls token has the same form as a class agnostic CAM-based saliency map}.

%In addition, pooling in vision transformers is defined as a weighted average of patch token embeddings, 
%where the weights are given by the raw attention map of the \cls token. This can be seen as reweighting, 
%or soft masking, of the embeddings before \emph{global average pooling} (GAP). By contrast, pooling in 
%convolutional networks is based on GAP only. We thus observe that \textbf{attention-based pooling is a form of masking in the feature space}. Masking, mostly in the input space, is common in interpretability methods~\cite{DBLP:journals/corr/abs-1910-01279} and their evaluation~\cite{DBLP:journals/corr/abs-1710-11063, petsiuk2018rise} to establish that a prediction is indeed due to a certain object of interest.

%Motivated by the above observations, we design an attention-based pooling mechanism as a replacement for 
%GAP in convolutional networks. Since this mechanism has the form of a CAM-based saliency map followed by 
%masking, we aim to study the effect of this network modification in interpreting the network predictions.

%Our pooling mechanism, called \emph{\OURS} (\emph{\Ours}), is implemented as a stream running in parallel 
%with the backbone network. At different stages of the network, it allows interaction between a \cls token 
%and patch embeddings by means of cross attention. The \cls token embedding is initialized as a learnable 
%parameter and, at the output of the stream, provides a global image representation for classification.

%We aim at post-hoc interpretability, therefore we keep the network and classifier frozen while learning 
%the parameters of \Ours. We then obtain CAM-based saliency maps by existing post-hoc methods for both GAP 
%and \Ours and compare their performance in terms of interpretability metrics as well as classification accuracy.

%More specifically, we make the following contributions:
%\begin{enumerate}[itemsep=2pt, parsep=0pt, topsep=3pt]
%    \item We show that attention-based pooling in vision transformers is the same as soft masking by a class agnostic CAM-based saliency map (\autoref{subsec:motiv}).
%    \item We design and inject an attention-based pooling mechanism into convolutional networks to replace GAP and study its effect on post-hoc interpretability (\autoref{subsec:CA-base}). This is a modification of the network but not of its training process.
%    \item We show that this mechanism helps explain a trained network by improving the performance of existing post-hoc interpretability methods as well as providing a class agnostic raw attention map (\autoref{sec:exp}).
%\end{enumerate}

\label{ch:castream}
%\input{tex_body/ch_castream/tex/intro}
\input{tex_body/ch_castream/tex/ca_defn}
\newpage
\input{tex_body/ch_castream/tex/ca_stream}
\newpage
\input{tex_body/ch_castream/tex/qual}
\newpage
\input{tex_body/ch_castream/tex/quant}
\newpage

