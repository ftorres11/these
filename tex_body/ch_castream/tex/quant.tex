%--------------------------------------------------------------------------------------------------
\section{Quantitative Results}
\label{sec:ca_quant}

Here we measure the effect of employing our \Ours approach to pool features \vs the baseline \gap 
on the faithfulness of explanations, using classification metrics for interpretability. Results are 
reported in \autoref{tab:intrecon-all} for ImageNet and  \autoref{tab:pascal} for CUB and Pascal VOC.\\

\input{tab/castream/tex/tab_interp_main.tex}

\noindent \autoref{tab:intrecon-all} shows that for different networks, CAM-based interpretability 
methods and dataset, \Ours provides consistent improvements over \gap in terms of AD, AG, AI and I 
metrics, while performing lower on D. \\

Deletion has raised concerns in previous works (\autoref{ch:opticam}, \cite{chefer2021transformer}). 
Indeed, it gradually replaces pixels by black, unlike insertion which starts from a blurred image. 
This poses the problem of \emph{out-of-distribution} (OOD) data (\cite{gomez2022metrics},
\cite{hase2021outofdistribution}, \cite{qiu2021resisting}), possibly introducing bias related to 
the shape of black regions \autocite{rong2022consistent}. Moreover, non-spread saliency maps tend to 
perform better (as seen on \autoref{ch:opticam}), which is likely the reason for lower performance. 

\input{tab/castream/tex/tab_joint_pascalcub.tex}

\noindent Results on CUB in \autoref{tab:pascal} show that our \Ours consistently provides 
improvements when the model is finetuned on a smaller fine-grained dataset.

\noindent Regarding Pascal VOC, the results for Score-CAM are similar to the ones on ImageNet and 
CUB, with consistent improvements on all metrics but Deletion. 
However, Grad-CAM and Grad-CAM++ only provide improvements on Average Gain and Average Increase. 
Average Drop, Insertion and Deletion are very similar.
In fact, Pascal VOC is a multi-class dataset and  our \Ours is class agnostic. Thus, the 
attention-based pooling is the same for different class for a given image, which reduces the 
benefit of our \Ours.

It is also interesting to observe the performance of Score-CAM, as it computes channel weights 
$\alpha_k^c$ in~\eq{sal} without using gradients. 
In gradient-based methods, channel weights are modified by \Ours due to modified backward gradient 
flow to features through cross attention blocks rather than \gap.
In Score-CAM however, channel weights are only modified in the forward class probabilities 
computation, due to attention.

\subsection{Classification accuracy}
\label{subsec:classification}
Classification accuracy, number of parameters and GFLOPs for both our \Ours and the baselines are 
reported in \autoref{tab:intrecon-acc}.

\input{tab/castream/tex/tab_interp_accmain.tex}

By adding our \Ours to the network, classification remains on par with the baseline. Importantly, 
the network including the classifier remains frozen and the features used for the global image 
representation remain fixed, meaning that any change in accuracy is due to the attention-based 
pooling mechanism. 


We further report the number of GFLOPs for one forward pass and the parameters count of both 
methods.
Our \Ours has little computation cost and the parameter overhead depends on the embedding dimension 
because of projection $W_\ell$ in~\eq{qk-layer} and is small in general, except for ResNet-50. 
Thus, with small overhead in resources, \Ours achieves superior explanations of the classifier 
predictions, while maintaining accuracy.

\subsection{Ablation}
\label{sec:gen_ablation}

We conduct ablation experiments on ResNet50 because of its modularity and ease of modification. We 
investigate the effect of the cross attention block design, the placement of the \Ours relative to 
the backbone network.
%The appendix further includes an extra experiments regarding class agnostic \vs class specific representations.

\paragraph{Cross attention block design}
Following transformers (\cite{vaswani2017attention}, \cite{dosovitskiy2020image}), it is possible to 
add more layers in the cross attention block. We consider a variant referred to as \PO, which uses 
linear projections $W_\ell^K, W_\ell^V \in \real^{d_\ell \times d_\ell}$ on the key and value

\begin{equation}
	\ca_\ell(\vq_\ell, F_\ell) \defn (F_\ell W^V_\ell)\tran h_\ell(F_\ell W^K_\ell \vq_\ell)\in 
    \real^{d_\ell},
\label{eq:proj_ca}
\end{equation}

while equation~\eq{qk-layer} remains.\\

\noindent Results are reported in \autoref{tab:dif_streams}. We observe that the stream made of 
vanilla CA blocks~\eq{CA} offers slightly better accuracy than projections~\eq{proj_ca}, while 
having less parameters. We also note that most of the computation takes place in the last residual 
stages, where the channel dimension is the largest. To keep our design simple, we choose the 
vanilla solution without projections~\eq{CA} by default.

\paragraph{\Ours placement}
\label{ab:placement}

To validate the design of \Ours, we measure the effect of its depth on its performance \vs the 
baseline \gap in terms of both classification accuracy / number of parameters and classification 
metrics for interpretability. In particular, we place the stream in parallel to the network $f$, 
starting at stage $\ell$ and running through stage $L$, the last stage of $f$, where $0 \le \ell 
\le L$. Results are reported in \autoref{tab:intrecog-resnet}.

\input{tab/castream/tex/tab_ca_design.tex}

\noindent From the interpretability metrics as well as accuracy, we observe that stream 
configurations that allow for iterative interaction with the network features obtain the best 
performance, although the effect of stream placement is small in general. In many cases, the 
lightest stream of only one cross attention block ($S_4-S_4$) is inferior to options allowing for 
more interaction. 
Since starting the stream at early stages has little effect on the number of parameters and 
performance is stable, we choose to start the stream in the first stage ($S_0-S_4$) by default.
\input{tab/castream/tex/tab_ab_incremental.tex}

\paragraph{Class-specific CLS}

%\textcolor{green}{
As discussed in \autoref{sec:ca_defn}, the formulation of single-query cross attention as a 
CAM-based saliency map ~\eq{att} is class agnostic (single channel weights $\alpha_k$), whereas the 
original CAM formulation ~\eq{sal} is class specific (channel weights $\alpha_k^c$ for given class 
of interest $c$). 
Here we consider a class specific extension of \Ours using one query vector per class. 
In particular, the stream is initialized by one learnable parameter $\vq_0^c$ per class $c$, but 
only one query (\cls token) embedding is forwarded along the stream. At training, $c$ is chosen 
according to the target class label, while at inference, the class predicted by the baseline 
classifier is used instead.

Results are reported in \autoref{tab:TokenvMatrix}. We observe that the class specific 
representation for \Ours provides no improvement over the class agnostic representation, despite 
the additional complexity and parameters. We thus choose the class agnostic representation by 
default. The class specific approach is similar to \autocite{touvron2021augmenting} in being able to 
generate class specific attention maps, although no fine-tuning is required in our case. 

\input{tab/castream/tex/tab_ab_cls_mat.tex}