\section{Related Work}

\subsection{Interpretability}

To open up the black-box behavior of deep neural networks, model interpretability is mainly investigated along two directions~\citep{lipton18, guidotti2018survey, zhang2021survey}:
\begin{enumerate}[itemsep=2pt, parsep=0pt, topsep=3pt]
	\item \emph{Post-hoc interpretability} considers the model as a black-box and provides explanations based on inputs and outputs, without modifying the model or its training process.
	\item \emph{Transparency} modifies the model or the training process to better explain the behavior of the inner parts of the model.
\end{enumerate}

\paragraph{Post-hoc interpretability}

Approaches can be grouped into a number of possibly overlapping categories. \emph{Gradient-based methods}~\citep{adebayo2018local, springenberg2014striving, baehrens2010explain, simonyan2013deep, smilkov2017smoothgrad, bach2015pixel, sundararajan2017axiomatic} use the gradient of a target class score with respect to the input to compute the contribution of different input regions to the prediction. \emph{CAM-based methods}~\citep{DBLP:journals/corr/abs-1910-01279, DBLP:journals/corr/abs-1710-11063, DBLP:journals/corr/SelvarajuDVCPB16, fu2020axiom, jiang2021layercam, ramaswamy2020ablation} compute saliency maps as a linear combination of feature maps, with different definitions for the weights. \emph{Occlusion or masking-based methods}~\citep{petsiuk2018rise, fong2017interpretable, fong2019understanding, schulz2020restricting, ribeiro2016should} apply a number of candidate masks in the input space, measure their effect on the prediction and then combine them into a saliency map. Masking in feature space has been explored too~\cite{schulz2020restricting}. Finally, \emph{learning-based methods}~\citep{chang2018explaining, dabkowski2017real, phang2020investigating, zolna2020classifier, schulz2020restricting} learn an additional network or branch on extra data to produce an explanation map for a given input. 

Our method shares similarities with learning-based methods. In particular, we train an additional branch on the same training data as the network we aim to explain, but we only use the standard classification loss and we do not provide any explanation as output. Our method is also similar to CAM-based and masking-based methods exactly because we show that attention-based pooling is the same as masking in the feature space with weights obtained by a CAM-based saliency map.

\paragraph{Transparency}

Approaches are grouped in a number of categories according to the type of the given explanation. \emph{Rule-based methods}~\citep{wu2018beyond, wu2020regional} train a decision tree as a surrogate regularization term to force a network to be easily approximated by a decision tree.
% or learn a set of logit rules as an explanation~\citep{azzolin2022global}.
\emph{Hidden semantics-based methods}~\citep{bau2017network, zhou2018interpreting, zhang2018interpretable, zhou2014object} aim to make a convolutional network learn disentangled hidden semantics with hierarchical structure or object-level concepts.
\emph{Prototype-based methods}~\citep{li2018deep, chen2019looks} learn a set of prototypes or parts as an intermediate representation in the network, which can be aligned with categories. \emph{Attribution-based methods}~\citep{ismail2021improving, Zhou_2022_BMVC, ross2017right, ghaeini2019saliency} usually modify the architecture of a network or the training process to help post-hoc methods produce better saliency maps. Unlike~\citep{ross2017right, ghaeini2019saliency}, saliency guided localization~\citep{Zhou_2022_BMVC} does not need ground truth explanations but replaces them with information bottleneck attribution~\citep{schulz2020restricting}. Finally, saliency-guided training~\citep{ismail2021improving} minimizes the KL divergence between the output of original and masked images.

Our method belongs to attribution-based methods. We introduce a learnable cross-attention stream into the network as a pooling mechanism to replace \gap. As a result, post-hoc attribution-based methods can provide a better explanation. Although the network is modified, the training process is not: the network and classifier are pretrained and kept frozen while we learn the paramaters of our stream.

%------------------------------------------------------------------------------

\subsection{Attention-based architectures}

Attention is a powerful mechanism that has been introduced into convolutional networks in several ocaasions~\citep{bello2019attention, ramachandran2019stand, shen2020global}. With the success of vision transformers (ViT)~\citep{dosovitskiy2020image}, fully attention-based architectures are now competitive with convolutional networks. To benefit from both self-attention and convolutional layers, some hybrid architectures employ convolutional layers before the vision transformer~\citep{graham2021levit,xiao2021early}. Others, such as Swin~\citep{liu2021swin} and PiT~\citep{heo2021rethinking}, introduce a pyramid structure to share local spatial information while reducing the spatial resolution, as in convolutional networks. 

Conformer~\citep{peng2021conformer} proposes a dual network structure to retrain and fuse local convolutional features with global representations. Our method merely provides a simple attention-based pooling mechanism inspired by transformers that can work with any architecture, even pretrained and frozen. SCOUTER~\citep{li2021scouter} uses slot attention~\cite{locatello2020object} to build a class specific pooling mechanism, which does not scale well to more than 20 classes. Our mechanism is based standard cross attention, it is class agnostic and scales up to 1000 classes, improving post-hoc interpretability without degrading classification accuracy. PatchConvNet~\citep{touvron2021augmenting} replaces global average pooling by an attention-based pooling layer. Our method is similar in this respect, but uses information collected from the entire network by a parallel processing stream. We introduce our mechanism into any convolutional network and study its effect on post-hoc interpretability.







