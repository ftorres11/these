\section{Method}

%------------------------------------------------------------------------------
\input{tex/mainfig}
%------------------------------------------------------------------------------

\subsection{Preliminaries and background}
\label{subsec:prelim}

\paragraph{Notation}

Let $f: \cX \rightarrow \real^C$ be a classifier network that maps an input image $\vx \in \cX$ to a logit vector $\vy= f(\vx) \in \real^C$, where $\cX$ is image space and $C$ is the number of classes. A class probability vector is obtained by $\vp = \softmax(\vy)$. The logit and probability of class $c$ are denoted by
$y^c$ and $p^c = \softmax(\vy)^c \defn e^{y^c} / \sum_j e^{y^j}$, respectively. Let $\vF_\ell \in \real^{w_\ell \times h_\ell \times d_\ell}$ be the feature tensor at layer $\ell$ of the network, where $w_\ell \times h_\ell$ is the spatial resolution and $d_\ell$ the embedding dimension, or number of channels. The feature map of channel $k$ is denoted by $F^k_\ell \in \real^{w_\ell \times h_\ell}$. By $\ell$ we may refer to an arbitrary layer of $f$ or a larger compositional block, \eg, a stage.

\paragraph{CAM-based saliency maps}

Given a class of interest $c$ and a layer $\ell$, we consider the saliency maps $S^c_\ell \in \real^{w_\ell \times h_\ell}$ given by the general formula
\begin{equation}
	S^c_\ell \defn h \left( \sum_k \alpha^c_k F^k_\ell \right),
\label{eq:sal}
\end{equation}
where $\alpha^c_k$ are weights defining a linear combination over channels and $h$ is an activation function. Assuming \emph{global average pooling} (GAP) of the last feature tensor $\vF_L$ followed by a linear classifier, CAM~\citep{zhou2016learning} is defined for the last layer $L$ only, with $h$ being the identity mapping and $\alpha^c_k$ the classifier weight connecting channel $k$ with class $c$. Grad-CAM~\citep{DBLP:journals/corr/SelvarajuDVCPB16} is a generalization of CAM defined for any architecture and layer $\ell$, with $h = \relu$ and weights
\begin{equation}
	\alpha^c_k \defn \gap \left( \pder{y^c}{F^k_\ell} \right).
\label{eq:gcam}
\end{equation}
% The \cls token is used for classification as it gathers information from all patches according to self-attention.

%------------------------------------------------------------------------------

\paragraph{Self-Attention}

Let $X_\ell \in \real^{t_\ell \times d_\ell}$ denote the sequence of token embeddings of a vision transformer~\cite{dosovitskiy2020image} at layer $\ell$, where $t_\ell \defn w_\ell h_\ell + 1$ is the number of tokens, including patch tokens and the \cls token, and $d_\ell$ is the embedding dimension. The \emph{query}, \emph{key} and \emph{value} matrices are defined as $Q = X_\ell W_Q$, $K = X_\ell W_K$, $V = X_\ell W_V \in \real^{t_\ell \times d_\ell}$, where $W_Q, W_K, W_V \in \real^{d_\ell \times d_\ell}$ are learnable linear projections. The \emph{attention matrix} $A \in \real^{t_\ell \times t_\ell}$ expresses pairwise dot-product similarities between queries (rows of $Q$) and keys (rows of $K$), normalized by softmax over rows
\begin{equation}
	A = \softmax \left( \frac{Q K\tran}{\sqrt{d_\ell}} \right).
\label{eq:attention}
\end{equation}
For each token, the \emph{self-attention} operation is then defined as an average of all values (rows of $V$) weighted by attention (the corresponding  row of $A$),
\begin{equation}
	\sa(X_\ell) \defn A V \in \real^{t_\ell \times d_\ell}.
\label{eq:SA}
\end{equation}
At the last layer $L$, the \cls token embedding is used as a global image representation for classification as it gathers information from all patches by weighted averaging, replacing \gap. Thus, at the last layer, it is only cross attention between \cls and the patch tokens that matters.

%------------------------------------------------------------------------------

\subsection{Motivation}
\label{subsec:motiv}

\paragraph{Cross attention}

Let matrix $F_\ell \in \real^{p_\ell \times d_\ell}$ be a reshaping of feature tensor $\vF_\ell$ at layer $\ell$, where $p_\ell \defn w_\ell h_\ell$ is the number of patch tokens without \cls, and let $\vq_\ell \in \real^{d_\ell}$ be the \cls token embedding at layer $\ell$. By focusing on the \emph{cross attention} only between the \cls (query) token $\vq_\ell$ and the patch (key) tokens $F_\ell$ and by ignoring projections $W_Q, W_K, W_V$ for simplicity, attention $A$~\eq{attention} is now a $1 \times p_\ell$ matrix that can be written as a vector $\va \in \real^{p_\ell}$
\begin{equation}
	\va = A\tran = \softmax \left( \frac{F_\ell \vq_\ell}{\sqrt{d_\ell}} \right).
\label{eq:cross-attention}
\end{equation}
Here, $F_\ell \vq_\ell$ expresses the pairwise similarities between the global \cls feature $\vq_\ell$ and the local patch features $F_\ell$. Now, by replacing $\vq_\ell$ by an arbitrary vector $\valpha \in \real^{d_\ell}$ and by writing the feature matrix as
$F_\ell = (\vf_\ell^1 \dots \vf_\ell^{d_\ell})$ where $\vf_\ell^k = \vect(F_\ell^k) \in \real^{p_\ell}$ for channel $k$, attention \eq{cross-attention} becomes
\begin{equation}
	\va = h_\ell (F_\ell \valpha) =
		h_\ell \left( \sum_k \alpha_k \vf_\ell^k \right).
\label{eq:connection}
\end{equation}
This takes the same form as~\eq{sal}, with feature maps $F_\ell^k$ being vectorized into $\vf_\ell^k$ and the nonlinearity is defined as $h_\ell(\vx) = \softmax(\vx / \sqrt{d_\ell})$. We thus observe the following.

\begin{quote}
	\emph{Pairwise similarities between one query and all patch token embeddings in cross attention are the same as a linear combination of feature maps in CAM-based saliency maps, where the weights are determined by the elements of the query.}
\end{quote}

As it stands, one difference between~\eq{sal} and~\eq{connection} is that~\eq{connection} is class agnostic, although it could be extended by using one query (weight) vector per class. For simplicity, we choose the class agnostic form  in the sequel. We also choose to have no query/key/value projections. We do experiment with extended versions in \autoref{sec:gen_ablation}.

\paragraph{Pooling, or masking}

We are thus motivated to integrate an attention mechanism into any network such that making a prediction and explaining (localizing) it are inherently connected. In particular, considering cross attention only between \cls and patch tokens~\eq{cross-attention}, equation~\eq{SA} becomes
\begin{align}
	\ca_\ell(\vq_\ell, F_\ell) \defn F_\ell\tran \va = F_\ell\tran h_\ell(F_\ell \vq_\ell) \in \real^{d_\ell}.
\label{eq:CA}
\end{align}
This is a weighted average of the local patch features $F_\ell$ with attention vector $\va$ expressing the weights. It can be used as a global image representation, replacing \gap.

Now, considering that $\va$ is obtained exactly as CAM-based saliency maps~\eq{connection}, we can think of the weighted averaging operation as \emph{soft masking}, similarly to occlusion (masking)-based methods~\citep{petsiuk2018rise,fong2017interpretable,fong2019understanding,schulz2020restricting,ribeiro2016should,DBLP:journals/corr/abs-1910-01279,zhang2023opti} and certain evaluation metrics~\cite{petsiuk2018rise}. It takes place in the feature space, whereas for interpretability it is common to mask in the input space. Masking in feature space is much more efficient as requires only one forward and has been explored for interpretability too~\cite{schulz2020restricting}.

%------------------------------------------------------------------------------

\subsection{Cross attention stream}
\label{subsec:CA-base}

We thus design a \emph{cross attention stream} (\Ours), which is a processing stream placed in parallel with any network $f$, interacting with image features at given points of $f$, using a cross attention mechanism to replace $\gap$. An example is shown in \autoref{fig:fig_method}, applied to a ResNet-based architecture.

In general, we use a classification token embedding to build a global image representation along the stream. It is initialized as a learnable parameter $\vq_0 \in \real^{d_0}$, and it is updated by interacting with local image features at points between blocks of $f$ where critical operations take place, such as change of spatial resolution or embedding dimension, \eg between stages for ResNet. In particular, we decompose $f$ as
\begin{equation}
	f = f_0 \circ \dots \circ f_L \circ \gap \circ g
\label{eq:f-decomp}
\end{equation}
such that features $F_\ell \in \real^{p_\ell \times d_\ell}$ of layer (stage) $\ell$ are initialized as $F_{-1} = \vx$ and updated according to
\begin{equation}
	F_\ell = f_\ell(F_{\ell-1})
\label{eq:f-layer}
\end{equation}
for $0 \le \ell \le L$. The last layer features $F_L$ are followed by $\gap$ and $g: \real^{d_L} \to \real^C$ is the classifier, mapping to the logit vector $\vy$. As in \autoref{subsec:motiv}, $p_\ell$ is the number of patch tokens and $d_\ell$ the embedding dimension of stage $\ell$.

In parallel, the sequence of classification token embeddings $\vq_\ell \in \real^{d_\ell}$ along the stream interacts with $F_\ell$ at each stage $\ell$. Referring to the global representation $\vq_\ell$ as \emph{query} or \cls and to the local image features $F_\ell$ as \emph{key} or patch embeddings, the interaction consists of cross attention followed by a linear projection $W_\ell \in \real^{d_{\ell+1} \times d_\ell}$ to account for changes of embedding dimension between the corresponding stages of $f$:
\begin{equation}
	\vq_{\ell+1} = W_\ell \cdot \ca_\ell(\vq_\ell, F_\ell),
\label{eq:qk-layer}
\end{equation}
for $0 \le \ell \le L$, where \ca is defined as in~\eq{CA}.

Image features $F_0, \dots, F_L$ do not change by injecting our \Ours into network $f$. However, the final global image representation and hence the prediction do change. In particular, at the last stage $L$, $\vq_{L+1}$ is used as a global image representation for classification, replacing \gap over $F_L$. The final prediction is $g(\vq_{L+1}) \in \real^C$. Unlike \gap, the weights of different image patches in the linear combination are non-uniform, enhancing the contribution of relevant patches in the prediction.
