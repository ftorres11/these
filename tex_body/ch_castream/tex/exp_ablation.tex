\subsection{Ablation}
\label{sec:gen_ablation}

We conduct ablation experiments on ResNet50 because of its modularity and ease of modification. We investigate the effect of the cross attention block design, the placement of the \Ours relative to the backbone network.
The appendix further includes an extra experiments regarding class agnostic \vs class specific representations.

%------------------------------------------------------------------------------

\paragraph{Cross attention block design}

Following transformers~\cite{NIPS2017_3f5ee243,dosovitskiy2020image}, it is possible to add more layers in the cross attention block. We consider a variant referred to as \PO, which uses linear projections $W_\ell^K, W_\ell^V \in \real^{d_\ell \times d_\ell}$ on the key and value
\begin{equation}
	\ca_\ell(\vq_\ell, F_\ell) \defn (F_\ell W^V_\ell)\tran h_\ell(F_\ell W^K_\ell \vq_\ell)\in \real^{d_\ell},
\label{eq:proj_ca}
\end{equation}
while equation~\eq{qk-layer} remains.

% We consider two variants. One, referred to as \PO, uses linear projections $W_\ell^K, W_\ell^V \in \real^{d_\ell \times d_\ell}$ on the key and value
% \begin{equation}
% 	\ca_\ell(\vq_\ell, F_\ell) \defn (F_\ell W^V_\ell)\tran h_\ell(F_\ell W^K_\ell \vq_\ell)\in \real^{d_\ell}.
% \label{eq:proj_ca}
% \end{equation}
% In another variant, referred to as \OM, an MLP follows each cross attention block, defined as
% \begin{equation}
% 	\mlp_\ell(\vq_\ell) = W_\ell^2 \gelu(W_\ell^1 \vq_\ell + b_\ell^1) + b_\ell^2,
% \label{eq:mlp_ca}
% \end{equation}
% where $W_\ell^1 \in \real^{2d_\ell \times d_\ell}$ and $W_\ell^1 \in \real^{d_\ell \times 2d_\ell}$. In both cases, equation~\eq{qk-layer} remains. The combination of the two variants is referred to as \POM.
% 
% We compare the effect on accuracy when adding one of the variants to the backbone (just after the last residual block). \autoref{tab:CA_variations} demonstrates this assessment.
% 
% %------------------------------------------------------------------------------
% \begin{table}
% \centering
% %\resizebox{\columnwidth}{!}{%
% \scriptsize
% \begin{tabular}{lcc}\\\toprule
% 	\Th{Block Type}&\Th{$\#$Params}&\Th{Acc}\\\midrule
% 	CA&4.20M&74.63\\
% 	Proj$\rightarrow$CA&12.58M&74.19\\
% 	CA$\rightarrow$MLP&20.97M&-\\
% 	Proj$\rightarrow$CA$\rightarrow$MLP&29.36M&-\\\bottomrule
% \end{tabular}
% %}
% \vspace{3pt}
% \caption{Comparison of classification performance when one cross attention block is added into the last residual block of ResNet-50.}
% \label{tab:CA_variations}
% \end{table}
% %------------------------------------------------------------------------------
% 
% We observe that the introduction of the basic design of the cross attention module outperforms the classification properties of that which uses projection of the input features to compute the subsequent representation. We hypothesize that maintaining the features without any projection allows the [class] token to better collect global information, as patch information is maintained in the same space that the already trained classifier relies on to perform classification. 
% On another hand, we also note that the introduction of an MLP to update said representation is not feasible given instability during training, for this reason we decide not to pursue further experimentation with this idea.

%------------------------------------------------------------------------------
\begin{table}
\centering
\scriptsize
%\resizebox{\columnwidth}{!}{%}
\begin{tabular}{lcc}\toprule
	\Th{Block Type}&\Th{$\#$Params}&\Th{Accuracy}\\\midrule
	\our&6.96M&74.70\\
	\PO&18.13M&74.41\\\bottomrule
\end{tabular}
%}
%\vspace{3pt}
\caption{\emph{Different cross attention block design for \Ours.} Classification accuracy and parameters using ResNet-50 on ImageNet. \Th{$\#$Param}: parameters of \Ours only.}
\label{tab:dif_streams}
\end{table}
%------------------------------------------------------------------------------

Results are reported in \autoref{tab:dif_streams}. We observe that the stream made of vanilla CA blocks~\eq{CA} offers slightly better accuracy than projections~\eq{proj_ca}, while having less parameters. We also note that most of the computation takes place in the last residual stages, where the channel dimension is the largest. To keep our design simple, we choose the vanilla solution without projections~\eq{CA} by default.

%------------------------------------------------------------------------------

\paragraph{\Ours placement}
\label{ab:placement}

To validate the design of \Ours, we measure the effect of its depth on its performance \vs the baseline \gap in terms of both classification accuracy / number of parameters and classification metrics for interpretability. In particular, we place the stream in parallel to the network $f$, starting at stage $\ell$ and running through stage $L$, the last stage of $f$, where $0 \le \ell \le L$. Results are reported in \autoref{tab:intrecog-resnet}.

%------------------------------------------------------------------------------
\begin{table}
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
%\resizebox{\columnwidth}{!}{%}
\begin{tabular}{lcccccc}\toprule
	\mc{7}{\Th{Accuracy and Parameters}}\\\midrule
	&\Th{Placement}&\mc{2}{\Th{CLS dim}}&\mc{2}{\Th{\#Param}}&\Th{Acc$\uparrow$}\\\midrule
	
	&$S_0-S_4$&\mc{2}{$64$}  &\mc{2}{6.96M}&\textbf{74.70}\\  % 6.963.264
	&$S_1-S_4$&\mc{2}{$256$} &\mc{2}{6.95M}&74.67\\           % 6.947.072
	&$S_2-S_4$&\mc{2}{$512$} &\mc{2}{6.82M}&74.67\\           % 6.816.256
	&$S_3-S_4$&\mc{2}{$1024$}&\mc{2}{6.29M}&74.67\\           % 6.292.480
	&$S_4-S_4$&\mc{2}{$2048$}&\mc{2}{4.20M}&74.63\\\midrule   % 4.196.352
	
	\mc{7}{\Th{Interpretability Metrics}}\\\midrule
	\Th{Method}&\Th{Placement}&\Th{AD$\downarrow$}&\Th{AG$\uparrow$}&\Th{AI$\uparrow$}&\Th{I$\uparrow$}&\Th{D$\downarrow$}\\\midrule
	
	\mr{5}{\Th{Grad-CAM}}&$S_0-S_4$&\textbf{12.54}&\textbf{22.67}&48.56&75.53&13.50\\ % Checked
		&$S_1-S_4$&12.69&22.65&48.31&75.53&13.41\\ % Checked
		&$S_2-S_4$&\textbf{12.54}&21.67&\textbf{48.58}&75.54&13.50\\ % Running
		&$S_3-S_4$&12.69&22.28&47.89&\textbf{75.55}&13.40\\ % Running
		&$S_4-S_4$&12.77&20.65&47.14&74.32&\textbf{13.37}\\\midrule % Checked
		
	\mr{5}{\Th{Grad-CAM++}}&$S_0-S_4$&13.99&19.29&44.60&75.21&13.78\\ % Checked
		&$S_1-S_4$&13.99&19.29&44.62&75.21&13.78\\ % Checked
		&$S_2-S_4$&13.71&\textbf{19.90}&\textbf{45.43}&75.34&13.50\\ % Running
		&$S_3-S_4$&13.69&19.61&45.04&\textbf{75.36}&13.50\\ % Running
		&$S_4-S_4$&\textbf{13.67}&18.36&44.40&74.19&\textbf{13.30}\\\midrule % Checked
		
	\mr{5}{\Th{Score-CAM}}&$S_0-S_4$&\textbf{7.09}&23.65&54.20&74.91&14.68\\ % Checked
		&$S_1-S_4$&\textbf{7.09}&23.65&54.20&74.92&14.68\\ % Checked
		&$S_2-S_4$&\textbf{7.09}&\textbf{23.66}&\textbf{54.21}&74.91&14.68\\ % Running
		&$S_3-S_4$&7.74&23.03&52.92&\textbf{74.97}&14.65\\ % Running
		&$S_4-S_4$&7.52&19.45&50.45&74.19&\textbf{14.46}\\\bottomrule % Checked
\end{tabular}
% }
%\vspace{3pt}
\caption{\emph{Effect of stream placement} on accuracy, parameters and interpretability metrics  for ResNet-50 on ImageNet. $S_\ell-S_L$: \Ours runs from stage $\ell$ to $L$ (last); \Th{$\#$Param}: parameters of \Ours only.}
\label{tab:intrecog-resnet}
\end{table}
%------------------------------------------------------------------------------

From the interpretability metrics as well as accuracy, we observe that stream configurations that allow for iterative interaction with the network features obtain the best performance, although the effect of stream placement is small in general. In many cases, the lightest stream of only one cross attention block ($S_4-S_4$) is inferior to options allowing for more interaction. 
Since starting the stream at early stages has little effect on the number of parameters and performance is stable, we choose to start the stream in the first stage ($S_0-S_4$) by default.

%------------------------------------------------------------------------------



%------------------------------------------------------------------------------

%% Top design, bottom interp metrics
%% More parts for experiments

% Ablation of histogram of diferences with extreme cases
