\section{Experiments}
\label{sec:exp}

We evaluate the interpretability and recognition capabilities of our approach. In particular, we generate explanations following current state-of-the art post-hoc interpretability methods derived from CAM~\cite{zhou2016learning}. We compare the properties of the backbone network $f$ with and without our \Ours, where $f$ is pretrained and fixed.

\subsection{Experimental setup}
\label{subsec:setup}

\paragraph{Training}

We train and evaluate our models on the ImageNet ILSVRC-2012 dataset~\cite{deng2009imagenet}, on the training and validation splits respectively. Thus, we experiment with ResNet-based architectures~\cite{he2016deep} such as ResNet-18 and ResNet-50, and ConvNeXt based architectures~\cite{liu2022convnet} such as ConvNeXt-Small and ConvNeXt-Base. We aim at learning our \Ours, generating a \cls token that interacts with feature maps at different stages of network $f$, to serve as an attention-based pooling mechanism in order to interpret the predictions of $f$. Therefore, we experiment with pretrained models\footnote{https://pytorch.org/vision/0.8/models.html}, that we keep frozen while the parameters of the \Ours are optimized. Details on training hyperparameters are given in the appendix.

Moreover, we present experiments on the bird dataset: CUB-200-2011 \cite{WahCUB_200_2011} and on PASCAL VOC 2012 dataset \cite{Everingham15}. Here the ResNet-50 network is fine-tuned to these dataset as baseline. Then, our \Ours is learned as for ImageNet.

\paragraph{Evaluation}

We employ existing post-hoc interpretability methods to generate saliency maps with and without \Ours and compare interpretability metrics as well as classification accuracy. Regarding interpretability methods, we use Grad-CAM~\cite{DBLP:journals/corr/SelvarajuDVCPB16}, Grad-CAM++~\cite{DBLP:journals/corr/abs-1710-11063} and ScoreCAM~\cite{DBLP:journals/corr/abs-1910-01279}. We note that the evaluation is performed on the entire validation set, unlike the previous approaches.

Following Opti-CAM~\cite{zhang2023opti}, we use a number of classification metrics for interpretability. In particular, we consider the changes in predictive power measured by \emph{average drop} (AD)~\cite{DBLP:journals/corr/abs-1710-11063} and \emph{average gain} (AG)~\cite{zhang2023opti}, the proportion of better explanations measured by \emph{average increase} (AI)~\cite{DBLP:journals/corr/abs-1710-11063} and the impact of different extent of masking measured by \emph{insertion} (I) and \emph{deletion} (D)~\citep{petsiuk2018rise}.

% For localization, we use metrics from the \emph{weakly-supervised object localization} (WSOL) task to measure the maximum overlap between the saliency map (or corresponding predicted bounding boxes) and ground truth bounding boxes, \ie \emph{official metric} (OM), \emph{localization error} (LE), \emph{pixel-wise $F_1$ score}, \emph{box accuracy} (BoxAcc)~\citep{choe2020evaluating} and \emph{saliency metric} (SM)~\citep{dabkowski2017real}. We also measure the localization of the pixel of maximum saliency by the \emph{standard pointing game} (SP)~\cite{zhang2018top} and the fraction of the saliency map within the ground truth bounding boxes by \emph{energy pointing game} (EP)~\citep{DBLP:journals/corr/abs-1910-01279}. We obtain the ground truth bounding boxes from the ILSVRC2014\footnote{\url{https://www.image-net.org/challenges/LSVRC/2014/index\#}} dataset. 


