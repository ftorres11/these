%--------------------------------------------------------------------------------------------------
\section{Conclusion}
\label{sec:ca_conclusion}
In this chapter we observe that attention-based pooling in transformers is the same as 
forming a class agnostic CAM-based saliency map. This map is used to mask the features before 
global average pooling, much like we mask inputs to confirm that the prediction is due to a certain 
object. This observation establishes that transformers have a built-in CAM-based interpretability 
mechanism and allows us to design a similar mechanism for convolutional networks. Masking in feature 
space is much more efficient than in the input space as it requires only one forward pass, although 
of course it is not equivalent because of interactions within the network.\\

\noindent Although the saliency maps obtained with our \Ours are not very different from those 
obtained with \gap, our approach improves a number of CAM-based interpretability methods on a number 
of convolutional networks according to most interpretability metrics, while preserving classification 
accuracy. By doing so, it also enhances the differences in performance between interpretability methods, 
facilitating their evaluation. Further study may be needed to improve the differentiation of saliency 
maps themselves, to possibly make a class specific representation more competitive and to apply the 
approach to more architectures, including transformers.