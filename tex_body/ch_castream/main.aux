\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{he2016deep,liu2022convnet}
\citation{zhou2016learning}
\citation{DBLP:journals/corr/SelvarajuDVCPB16,DBLP:journals/corr/abs-1710-11063,DBLP:journals/corr/abs-1910-01279}
\citation{dosovitskiy2020image}
\citation{dino}
\citation{chefer2021transformer}
\citation{DBLP:journals/corr/abs-1910-01279}
\citation{DBLP:journals/corr/abs-1710-11063,petsiuk2018rise}
\citation{lipton18,guidotti2018survey,zhang2021survey}
\citation{adebayo2018local,springenberg2014striving,baehrens2010explain,simonyan2013deep,smilkov2017smoothgrad,bach2015pixel,sundararajan2017axiomatic}
\citation{DBLP:journals/corr/abs-1910-01279,DBLP:journals/corr/abs-1710-11063,DBLP:journals/corr/SelvarajuDVCPB16,fu2020axiom,jiang2021layercam,ramaswamy2020ablation}
\citation{petsiuk2018rise,fong2017interpretable,fong2019understanding,schulz2020restricting,ribeiro2016should}
\citation{schulz2020restricting}
\citation{chang2018explaining,dabkowski2017real,phang2020investigating,zolna2020classifier,schulz2020restricting}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{\hskip -1em.~Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.\nobreakspace  {}Interpretability}{1}{subsection.2.1}}
\@writefile{toc}{\contentsline {paragraph}{Post-hoc interpretability}{1}{section*.1}}
\citation{wu2018beyond,wu2020regional}
\citation{bau2017network,zhou2018interpreting,zhang2018interpretable,zhou2014object}
\citation{li2018deep,chen2019looks}
\citation{ismail2021improving,Zhou_2022_BMVC,ross2017right,ghaeini2019saliency}
\citation{ross2017right,ghaeini2019saliency}
\citation{Zhou_2022_BMVC}
\citation{schulz2020restricting}
\citation{ismail2021improving}
\citation{bello2019attention,ramachandran2019stand,shen2020global}
\citation{dosovitskiy2020image}
\citation{graham2021levit,xiao2021early}
\citation{liu2021swin}
\citation{heo2021rethinking}
\citation{peng2021conformer}
\citation{li2021scouter}
\citation{locatello2020object}
\citation{touvron2021augmenting}
\citation{dosovitskiy2020image}
\citation{zhou2016learning}
\citation{dosovitskiy2020image}
\citation{zhou2016learning}
\citation{zhou2016learning}
\citation{DBLP:journals/corr/SelvarajuDVCPB16}
\citation{dosovitskiy2020image}
\@writefile{toc}{\contentsline {paragraph}{Transparency}{2}{section*.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.\nobreakspace  {}Attention-based architectures}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Method}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Preliminaries and background}{2}{subsection.3.1}}
\newlabel{subsec:prelim}{{3.1}{2}{\hskip -1em.~Preliminaries and background}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Notation}{2}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{CAM-based saliency maps}{2}{section*.4}}
\newlabel{eq:sal}{{1}{2}{CAM-based saliency maps}{equation.3.1}{}}
\newlabel{eq:gcam}{{2}{2}{CAM-based saliency maps}{equation.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Self-Attention}{2}{section*.5}}
\citation{petsiuk2018rise,fong2017interpretable,fong2019understanding,schulz2020restricting,ribeiro2016should,DBLP:journals/corr/abs-1910-01279,zhang2023opti}
\citation{DBLP:journals/corr/abs-1710-11063,petsiuk2018rise}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \emph  {Visualization of eq.\nobreakspace  {}(\ref  {eq:connection}).} On the left, a feature tensor $\mathbf  {F}\in \mathbb  {R}^{w \times h \times d}$ is multiplied by the vector ${\boldsymbol  {\alpha }}\in \mathbb  {R}^d$ in the channel dimension, like in $1 \times 1$ convolution, where $w \times h$ is the spatial resolution and $d$ is the number of channels. This is \emph  {cross attention} (CA)\nobreakspace  {}\cite  {dosovitskiy2020image} between the query ${\boldsymbol  {\alpha }}$ and the key $\mathbf  {F}$. On the right, a linear combination of feature maps $F^1, \dots  , F^d \in \mathbb  {R}^{w \times h}$ is taken with weights $\alpha _1, \dots  , \alpha _d$. This is a \emph  {class activation mapping} (CAM)\nobreakspace  {}\cite  {zhou2016learning} with class agnostic weights. Eq.\nobreakspace  {}(\ref  {eq:connection}) expresses the fact that these two quantities are the same, provided that ${\boldsymbol  {\alpha }}= (\alpha _1, \dots  , \alpha _d)$ and $\mathbf  {F}$ is reshaped as $F = (\mathbf  {f}^1 \dots  \mathbf  {f}^d) \in \mathbb  {R}^{p \times d}$, where $p = wh$ and $\mathbf  {f}^k = \operatorname  {vec}(F^k) \in \mathbb  {R}^{p}$ is the vectorized feature map of channel $k$.}}{3}{figure.1}}
\newlabel{fig:connection}{{1}{3}{\emph {Visualization of eq.~\eq {connection}.} On the left, a feature tensor $\vF \in \real ^{w \times h \times d}$ is multiplied by the vector $\valpha \in \real ^d$ in the channel dimension, like in $1 \times 1$ convolution, where $w \times h$ is the spatial resolution and $d$ is the number of channels. This is \emph {cross attention} (CA)~\cite {dosovitskiy2020image} between the query $\valpha $ and the key $\vF $. On the right, a linear combination of feature maps $F^1, \dots , F^d \in \real ^{w \times h}$ is taken with weights $\alpha _1, \dots , \alpha _d$. This is a \emph {class activation mapping} (CAM)~\cite {zhou2016learning} with class agnostic weights. Eq.~\eq {connection} expresses the fact that these two quantities are the same, provided that $\valpha = (\alpha _1, \dots , \alpha _d)$ and $\vF $ is reshaped as $F = (\vf ^1 \dots \vf ^d) \in \real ^{p \times d}$, where $p = wh$ and $\vf ^k = \vect (F^k) \in \real ^{p}$ is the vectorized feature map of channel $k$}{figure.1}{}}
\newlabel{eq:attention}{{3}{3}{Self-Attention}{equation.3.3}{}}
\newlabel{eq:SA}{{4}{3}{Self-Attention}{equation.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Motivation}{3}{subsection.3.2}}
\newlabel{subsec:motiv}{{3.2}{3}{\hskip -1em.~Motivation}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Cross attention}{3}{section*.6}}
\newlabel{eq:cross-attention}{{5}{3}{Cross attention}{equation.3.5}{}}
\newlabel{eq:connection}{{6}{3}{Cross attention}{equation.3.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Pooling, or masking}{3}{section*.7}}
\newlabel{eq:CA}{{7}{3}{Pooling, or masking}{equation.3.7}{}}
\newlabel{eq:CA-gap}{{8}{3}{Pooling, or masking}{equation.3.8}{}}
\citation{DBLP:journals/corr/SelvarajuDVCPB16}
\citation{DBLP:journals/corr/SelvarajuDVCPB16}
\citation{zhou2016learning}
\citation{deng2009imagenet}
\citation{he2016deep}
\citation{liu2022convnet}
\citation{WahCUB_200_2011}
\citation{Everingham15}
\citation{DBLP:journals/corr/SelvarajuDVCPB16}
\citation{DBLP:journals/corr/abs-1710-11063}
\citation{DBLP:journals/corr/abs-1910-01279}
\citation{zhang2023opti}
\citation{DBLP:journals/corr/abs-1710-11063}
\citation{zhang2023opti}
\citation{DBLP:journals/corr/abs-1710-11063}
\citation{petsiuk2018rise}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Cross attention stream}{4}{subsection.3.3}}
\newlabel{subsec:CA-base}{{3.3}{4}{\hskip -1em.~Cross attention stream}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Architecture}{4}{section*.8}}
\newlabel{eq:f-decomp}{{9}{4}{Architecture}{equation.3.9}{}}
\newlabel{eq:f-layer}{{10}{4}{Architecture}{equation.3.10}{}}
\newlabel{eq:qk-layer}{{11}{4}{Architecture}{equation.3.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Training}{4}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{Inference}{4}{section*.10}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Experiments}{4}{section.4}}
\newlabel{sec:exp}{{4}{4}{\hskip -1em.~Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}Experimental setup}{4}{subsection.4.1}}
\newlabel{subsec:setup}{{4.1}{4}{\hskip -1em.~Experimental setup}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Training}{4}{section*.11}}
\@writefile{toc}{\contentsline {paragraph}{Evaluation}{4}{section*.12}}
\citation{quattoni2009recognizing}
\citation{quattoni2009recognizing}
\citation{quattoni2009recognizing}
\citation{chefer2021transformer,zhang2023opti}
\citation{gomez2022metrics,hase2021outofdistribution,qiu2021resisting}
\citation{rong2022consistent}
\citation{zhang2023opti}
\pgfsyspdfmark {pgfid1}{17658709}{44989231}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \emph  {{Cross Attention Stream}\xspace  ({CA-Stream}\xspace  ) applied to ResNet-based architectures.} Given a network $f$, we replace global average pooling ({\textsc  {gap}}\xspace  ) by a learned, attention-based pooling mechanism implemented as a stream in parallel to $f$. The feature tensor $F_\ell \in \mathbb  {R}^{p_\ell \times d_\ell }$ (\emph  {key}) obtained by stage Res-$\ell $ of $f$ interacts with a {\textsc  {cls}}\xspace  token (\emph  {query}) embedding $\mathbf  {q}_\ell \in \mathbb  {R}^{d_\ell }$ in block CA-$\ell $, which contains cross attention\nobreakspace  {}(\ref  {eq:CA}) followed by a linear projection\nobreakspace  {}(\ref  {eq:qk-layer}) to adapt to the dimension of $F_{\ell +1}$. Here, $p_\ell $ is the number of patches (spatial resolution) and $d_\ell $ the embedding dimension. The query is initialized by a learnable parameter $\mathbf  {q}_0 \in \mathbb  {R}^{d_0}$, while the output $\mathbf  {q}_5$ of the last cross attention block is used as a global image representation into the classifier. The network and classifier are pretrained and kept frozen while the parameters of {CA-Stream}\xspace  are learned. At inference, we use existing post-hoc interpretability methods like Grad-CAM\nobreakspace  {}\citep  {DBLP:journals/corr/SelvarajuDVCPB16} to obtain saliency maps for both the baseline {\textsc  {gap}}\xspace  and our {CA-Stream}\xspace  . We compare interpretability metrics as well as accuracy.}}{5}{figure.2}}
\newlabel{fig:fig_method}{{2}{5}{\emph {\OURS (\Ours ) applied to ResNet-based architectures.} Given a network $f$, we replace global average pooling (\gap ) by a learned, attention-based pooling mechanism implemented as a stream in parallel to $f$. The feature tensor $F_\ell \in \real ^{p_\ell \times d_\ell }$ (\emph {key}) obtained by stage Res-$\ell $ of $f$ interacts with a \cls token (\emph {query}) embedding $\vq _\ell \in \real ^{d_\ell }$ in block CA-$\ell $, which contains cross attention~\eq {CA} followed by a linear projection~\eq {qk-layer} to adapt to the dimension of $F_{\ell +1}$. Here, $p_\ell $ is the number of patches (spatial resolution) and $d_\ell $ the embedding dimension. The query is initialized by a learnable parameter $\vq _0 \in \real ^{d_0}$, while the output $\vq _5$ of the last cross attention block is used as a global image representation into the classifier. The network and classifier are pretrained and kept frozen while the parameters of \Ours are learned. At inference, we use existing post-hoc interpretability methods like Grad-CAM~\citep {DBLP:journals/corr/SelvarajuDVCPB16} to obtain saliency maps for both the baseline \gap and our \Ours . We compare interpretability metrics as well as accuracy}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.\nobreakspace  {}Qualitative evaluation}{5}{subsection.4.2}}
\newlabel{subsec:vinspection}{{4.2}{5}{\hskip -1em.~Qualitative evaluation}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}\hskip -1em.\nobreakspace  {}Interpretabity metrics}{5}{subsection.4.3}}
\newlabel{subsec:interecon}{{4.3}{5}{\hskip -1em.~Interpretabity metrics}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}\hskip -1em.\nobreakspace  {}Classification accuracy}{5}{subsection.4.4}}
\newlabel{subsec:classification}{{4.4}{5}{\hskip -1em.~Classification accuracy}{subsection.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of saliency maps generated by different CAM-based methods, using GAP and our {CA-Stream}\xspace  , on ImageNet images. The raw attention is the one used for pooling by {CA-Stream}\xspace  .}}{6}{figure.3}}
\newlabel{fig:compmethods}{{3}{6}{Comparison of saliency maps generated by different CAM-based methods, using GAP and our \Ours , on ImageNet images. The raw attention is the one used for pooling by \Ours }{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Raw attention maps obtained from our {CA-Stream}\xspace  on images of the MIT 67 Scenes dataset\nobreakspace  {}\cite  {quattoni2009recognizing} on classes that do not exist in ImageNet. The network sees them at inference for the first time.}}{6}{figure.4}}
\newlabel{fig:enter-label}{{4}{6}{Raw attention maps obtained from our \Ours on images of the MIT 67 Scenes dataset~\cite {quattoni2009recognizing} on classes that do not exist in ImageNet. The network sees them at inference for the first time}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}\hskip -1em.\nobreakspace  {}Ablation}{6}{subsection.4.5}}
\newlabel{sec:gen_ablation}{{4.5}{6}{\hskip -1em.~Ablation}{subsection.4.5}{}}
\citation{NIPS2017_3f5ee243,dosovitskiy2020image}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \emph  {Accuracy, parameters and interpretability metrics} of {CA-Stream}\xspace  \emph  {vs}\onedot  baseline {\textsc  {gap}}\xspace  for different networks and interpretability methods on ImageNet. \textsc  {$\#$Param}: total parameters; \textsc  {Param$\%$}: percentage of {CA-Stream}\xspace  parameters relative to backbone.}}{7}{table.1}}
\newlabel{tab:intrecon-all}{{1}{7}{\emph {Accuracy, parameters and interpretability metrics} of \Ours \vs baseline \gap for different networks and interpretability methods on ImageNet. \Th {$\#$Param}: total parameters; \Th {Param$\%$}: percentage of \Ours parameters relative to backbone}{table.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Cross attention block design}{7}{section*.13}}
\newlabel{eq:proj_ca}{{12}{7}{Cross attention block design}{equation.4.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Accuracy, respectively mean Average Precision, and interpretability metrics of {CA-Stream}\xspace  \emph  {vs}\onedot  baseline {\textsc  {gap}}\xspace  for ResNet-50 on CUB and Pascal dataset.}}{7}{table.2}}
\newlabel{tab:pascal}{{2}{7}{Accuracy, respectively mean Average Precision, and interpretability metrics of \Ours \vs baseline \gap for ResNet-50 on CUB and Pascal dataset}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \emph  {Different cross attention block design for {CA-Stream}\xspace  .} Classification accuracy and parameters using ResNet-50 on ImageNet. \textsc  {$\#$Param}: parameters of {CA-Stream}\xspace  only.}}{7}{table.3}}
\newlabel{tab:dif_streams}{{3}{7}{\emph {Different cross attention block design for \Ours .} Classification accuracy and parameters using ResNet-50 on ImageNet. \Th {$\#$Param}: parameters of \Ours only}{table.3}{}}
\newlabel{ab:placement}{{4.5}{7}{\Ours placement}{section*.14}{}}
\@writefile{toc}{\contentsline {paragraph}{{CA-Stream}\xspace  placement}{7}{section*.14}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \emph  {Effect of stream placement} on accuracy, parameters and interpretability metrics for ResNet-50 on ImageNet. $S_\ell -S_L$: {CA-Stream}\xspace  runs from stage $\ell $ to $L$ (last); \textsc  {$\#$Param}: parameters of {CA-Stream}\xspace  only.}}{8}{table.4}}
\newlabel{tab:intrecog-resnet}{{4}{8}{\emph {Effect of stream placement} on accuracy, parameters and interpretability metrics for ResNet-50 on ImageNet. $S_\ell -S_L$: \Ours runs from stage $\ell $ to $L$ (last); \Th {$\#$Param}: parameters of \Ours only}{table.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Conclusion}{8}{section.5}}
\bibstyle{ieee_fullname}
\bibdata{egbib}
\@writefile{toc}{\contentsline {section}{\numberline {A1}\hskip -1em.\nobreakspace  {}More on experimental setup}{1}{appendix.A}}
\@writefile{toc}{\contentsline {paragraph}{Implementation details}{1}{section*.15}}
\@writefile{toc}{\contentsline {section}{\numberline {A2}\hskip -1em.\nobreakspace  {}More ablations}{1}{appendix.B}}
\@writefile{toc}{\contentsline {paragraph}{Class-specific CLS}{1}{section*.16}}
\@writefile{lot}{\contentsline {table}{\numberline {A5}{\ignorespaces \emph  {Effect of class agnostic \emph  {vs}\onedot  class specific representation} on accuracy, parameters and interpretability metrics of {CA-Stream}\xspace  for ResNet-50 and different interpretability methods on ImageNet. \textsc  {$\#$Param}: parameters of {CA-Stream}\xspace  only.}}{1}{table.5}}
\newlabel{tab:TokenvMatrix}{{A5}{1}{\emph {Effect of class agnostic \vs class specific representation} on accuracy, parameters and interpretability metrics of \Ours for ResNet-50 and different interpretability methods on ImageNet. \Th {$\#$Param}: parameters of \Ours only}{table.5}{}}
