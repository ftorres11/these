\section{Experiments}
%We evaluate our approach to study the interpretability properties it presents and the effect on classification performance.
In this section, we evaluate the interpretability and recognition properties of our approach. For this goal,  we generate explanations following current state-of-the art post-hoc interpretability methods derived from CAM\cite{DBLP:journals/corr/HeZRS15}; namely Grad-CAM\cite{DBLP:journals/corr/SelvarajuDVCPB16}, Grad-CAM++\cite{DBLP:journals/corr/abs-1710-11063} and most recently ScoreCAM\cite{DBLP:journals/corr/abs-1910-01279}. First, on Section \ref{subsec:setup} we briefly describe the general training regime used to train our approach alongside its variations. Next, we conduct qualitative evaluation for our explanations found in Section \ref{subsec:vinspection}. Afterwards, we assess the fairness of these explanations on image recognition in Section \ref{subsec:interecon}. Moreover, on Section \ref{subsec:loceval} we study their object localization capabilities. On Section \ref{subsect:classification} we compare the classification performance derived from the addition of this attention mechanism to existing models. Lastly, on Section \ref{sec:gen_ablation} we present experiments regarding the design of the CLS Stream and some of its properties.

\subsection{Experimental Setup}\label{subsec:setup}
%We train and evaluate our models on the ImageNet ILSVRC2012 dataset\cite{deng2009imagenet} on the training and validation splits respectively. Since we aim at learning our [CLS] token alongside its interactions with feature maps at different stages of a network; we experiment with pretrained models\footnote{https://pytorch.org/vision/0.8/models.html}, that are used to optimize the CLS stream. During this training phase we learn the parameters of this stream, maintaining the backbone properties intact and leveraging the classifier's predictive power with our representations. Details of this training phase are given in the supplementary material.\\
We train and evaluate our models on the ImageNet ILSVRC-2012 dataset\cite{deng2009imagenet} on the training and validation splits respectively. Since we aim at learning our [CLS] token alongside its interactions with feature maps at different stages of a network; we experiment with pretrained models\footnote{https://pytorch.org/vision/0.8/models.html}, that are used to optimize the CLS stream. During this training phase we learn the parameters of this branch, leveraging its interactions with the backbone and the classifier's predictive power with our representations. Further details of this training phase are given in the supplementary material. \textbf{Maybe here the main recipe for ResNet knowing that its used throughout the ablation}\\

To evaluate our approach and in contrast to similar alternatives assessing interpretability; we use the entirety of the validation set, gaining complete insight on the real performance of our proposed method and its comparisons. To do so, we generate saliency maps of the ensemble of model and CLS stream, that are contrasted with those generated by the application the aforementioned interpretability methods on the model alone.\\  %More details of the experimental set-up
