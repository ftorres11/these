\section{Introduction}


%power of MHSA comes from : 1. weak inductive bias = more degrees of freedom
% 2. large context window help ? or not ?

%also less sensible to attacks of high frequency. MHSA smoothing ?


Since the re-popularization of deep learning in computer vision a decade ago, a large effort has been dedicated to finding efficient Neural Networks (NN) architectures.
While Convolutional NN (CNN) reigned supreme for a long time, the competition is now raising with Transformers.
Indeed CNNs apply a series of layers of convolutional filters, aggregate the spatial information while increasing the number of filters, resulting in dense global descriptors used for recognition.
Convolutions are particularly adequate to model image structure and there is no questioning about the great capabilities of such networks.
However, Transformers are bringing self-attention modules and new learning paradigms that can improve models further.

Self-attention layers are the key feature of Transformers. 
The input representation is projected into three matrices that are used to compute the attention map that is applied to the values matrix.
These modules allow higher order information to be diffused over the patches.
However, part of the information localization is lost on the way.

In this work, we aim at adapting self-attention and particularly the CLS \textcolor{green}{mechanism} into a CNN architecture.
We adopt a cross-attention module and build a stream to include attention information at several key stages of CNN. 
In the latest stage, our module replaces the global average pooling typically used to obtain attention-based pooling.

We propose to use our stream in a post-hoc interpretability scenario, meaning that we will use a fixed pre-trained network and aim to provide better interpretability using existing methods.
Moreover, our stream provides attention maps that can be used to understand what the networks attend to without any label relations.

More specifically we make the following contribution:
\begin{itemize}
    \item We propose a cross-attention module, derived from self-attention that focuses on the CLS aggregation.
    \item Our modules are built together in a stream that can connect and adapt to the layers of a CNN architecture.
    \item The trained stream helps explain a trained network, by providing an attention map independent of any categories, as well as improving the performance of existing post-hoc interpretability methods.
\end{itemize}

After a review of previous work, we present our method and experimental results.




