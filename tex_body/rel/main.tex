\chapter{Literature Review: Image Recognition Models and Interpretability}
\label{ch:rel}
\chaptertoc{}
\section{Introduction}
The capacities of image recognition models have improved with constant development, 
closely aligned with the increase in computing power. This evolution has lead to steady 
improvements in both performance and complexity. On its early approaches, image recognition models 
relied on handcrafted feature extraction methods  in conjuction with traditional machine learning 
algorithms. However, this reliance on these features ultimately limits these methodologies' 
capabilities to capture intricate visual patterns.
On particular, one approach that had a strong initial impact and high performance was \gls{hog} 
\autocite{dalal2005histograms}. In this methodology, gradient information was used to train a 
\gls{svm} to perform pedestrian recognition. While achieving high performance in the dataset it was 
designed, \gls{hog} ultimatately failed in data collections where complexity was higher 
\autocite{5975165}.\\
Furthermore, with the resurgence of convolutional neural networks and the ensuing advent of 
deep learning, a paradigm shift in this field occurred. This transition led to the elimination of 
the need for handcrafted feature extraction; instead, deep learning allowed CNNs to act 
as both feature extractor and classifiers on themselves. Moreover, based on the aggregation of 
convolutions as their fundamental units, a \gls{cnn} is able to learn hierarchichal 
representations of data, extracting intricate features directly. 
Consequently, this shift resulted in improvements in accuracy and performance in various image 
recognition tasks. \\

\noindent In \autoref{rel:sub_background} we explore some of the most important early approaches based 
on machine learning designed towards image recognition. Additionally in \autoref{rel:sub_cnn}, we 
introduce the basis of deep learning and CNNs. Moving on, in \autoref{rel:sub_att} we make mention 
of the Transformer architecture, its basis and how it has reshaped the landscape of image 
recognition. Finally in \autoref{rel:sub_hybrid} we display approaches that make use of combinations 
of the last two forementioned approaches.\\


\noindent However, with the adoption of deep image recognition into society; understanding the 
innerworkings of these models has become a top priority. We shine light into some fields where 
this is the case:
\begin{itemize}
    \item \textbf{Facial Recognition} Facial recognition as a fine grained classification task, 
    that can be mostly associated with identification and re-identification of individuals. 
    In this aspect, understanding model predictions is associated with accountability, ethical 
    considerations and safety.
    (\cite{selinger2020inconsentability}, \cite{andrejevic2020facial}).
    \item \textbf{Automated Medical Diagnosis} Regarding medical imaging, the education 
    required to read and provide analysis, often requires experience based 
    on personal expertise \autocite{nakashima2013visual}. Examples of automated diagnosis encompas 
    melanoma detection, bone age assessment and most recently, COVID-19 diagnosis (\cite{yu2016automated}, 
    \cite{BoNet2019hand}, \cite{huang2021artificial}). The medical domain is of special care 
    as human lives are directly at stake.
    \item \textbf{Self-driving Vehicles} Over the past decade, advancements in this field have lead to 
    discussions regarding the impact of the adoption of these kind of vehicles within smart capacities 
    (\cite{duarte2018impact}, \cite{millard2018pedestrians}). Nevertheless their 
    navigation is not completely perfect and it is also possible to attack it, leading to possible 
    traffic accidents \autocite{dixit2016autonomous}.
\end{itemize}

We observe that interpretability needs in these fields and real world applications follows 
Lipton's discussion on the \textit{Desiderata of Interpretability Research} 
\autocite{mythos_interp}. In this aspect, we expect that the explanations that any given 
approach help us \textbf{trust} any model. Conversely, we expect a model to be explained in a 
\textbf{causal} manner according to its explanations. On another hand, 
we expect these remarks be \textbf{informative} and shine light on similar examples that the 
model processes. Finally, we expect interpretable explanations to guarantee that the outputs 
of a model are \textbf{fair} and \textbf{ethical}.

With these properties in mind leading the desiredata of interpretability study, we further 
investigate in this work some of the most important works on interpretability. In 
\autoref{rel:sec_int} we explore the preliminary works of this field. Delving deeper, in 
\autoref{rel:sub_transp} we discuss efforts aimed at transparency of machine learning approaches. 
In contrast, \autoref{rel:sub_post} we explore and study 
some of the most relevant studies on post-hoc interpretability. Finally, in \autoref{rel:sub_evl}, 
we outline the evaluation metrics used for evaluation the aforementioned works.
%\addcontentsline{toc}{chapter}{Literature Review}
\input{tex_body/rel/tex/imrecon}
\newpage
\input{tex_body/rel/tex/interp}
\newpage
\input{tex_body/rel/tex/evaluating}