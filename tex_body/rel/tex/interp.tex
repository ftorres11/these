%--------------------------------------------------------------------------------------------------
\section*{Interpretability}
\label{rel:sec_int}
\addcontentsline{toc}{section}{Interpretability}

\subsection*{Transparency}
\label{rel:sub_transp}

\subsection*{Post-Hoc Interpretability}
\label{rel:sub_post}

\paragraph{Notation}
\label{sec:oc_notation}

Consider a classifier network $f: \cX \to \real^C$ that maps an input image $\mathbf{u} \in \cX$ to a 
logit vector $\vy = f(\mathbf{u}) \in \real^C$, where $\cX$ is the image space and $C$ is the number 
of classes. We denote by $y_c = f(\mathbf{u})_c$ the predicted logit and by $p_c = \softmax(\vy)_c 
\defn e^{y_c} / \sum_j e^{y_j}$ the predicted probability for class $c$. For layer $\ell$ 
with $K_\ell$ channels, we denote by $A^k_\ell = f^k_\ell(\mathbf{u}) \in \real^{h_\ell \times w_\ell}$ 
the feature map for channel $k \in \{1,\dots,K_\ell\}$, with spatial resolution $h_\ell \times 
w_\ell$. Because of $\relu$ non-linearities, we assume that feature maps are non-negative. 
Similarly, we denote by $S_\ell \in \real^{h_\ell \times w_\ell}$ a 2D saliency map.

%--------------------------------------------------------------------------------------------------

\paragraph{Background: CAM-based saliency maps}
\label{sec:oc_back}

Given a layer $\ell$ and a class of interest $c$, we consider saliency maps given by the general 
formula
\begin{equation}
	S^c_\ell(\mathbf{u}) \defn h \left( \sum_k w^c_k A^k_\ell \right),
\label{eq:sal}
\end{equation}
where $w^c_k$ are weights defining a linear combination over channels and $h$ is an activation 
function. CAM \parencite{zhou2016learning} is defined for the last layer $L$ only with $h$ being the 
identity mapping and $w^c_k$ being the classifier weight connecting the $k$-th channel with 
class $c$. Grad-CAM \parencite{selvaraju2017grad} is defined for any layer $\ell$ with $h = \relu$ and 
weights
\begin{equation}
	w^c_k \defn \gap \left( \pder{y_c}{A^k_\ell} \right),
\label{eq:gcam}
\end{equation}
where $\gap$ is global average pooling.
% and $\softmax(\vy)_c = e^{y_c} / \sum_j e^{y_j}$ is the predicted probability of class $c$.
The motivation for $\relu$ is that we are only interested in features that have a positive effect 
on the class of interest, \ie pixels whose intensity should be increased in order to increase $y_c$.

Score-CAM \parencite{wang2020score} is also defined for any layer $\ell$ with $h = \relu$ and weights 
$w^c_k \defn \softmax(\mathbf{u}^c)_k$.  Softmax normalization considers positive channel contributions 
only and attends to few feature maps.
%that \alert{produce less highlighted areas in the saliency map}. \iavr{Last part unclear.}
Here, vector $\mathbf{u}^c \in \real^{K_\ell}$ measures the increase in confidence for class $c$ that 
compares a known baseline image $\mathbf{u}_b$ with the input image $\mathbf{u}$ masked according to feature 
map $A^k_\ell$, for all channels $k$:

\begin{equation}
	u^c_k \defn f(\mathbf{u} \odot n(\operatorname{up}( A^k_\ell )))_c - f(\mathbf{u}_b)_c,
\label{eq:s-cam}
\end{equation}

where $\odot$ is the Hadamard product. For this to work, the feature map $A^k_\ell$ is adapted
 to $\mathbf{u}$ first$:\operatorname{up}$ denotes upsampling to the spatial resolution of $\mathbf{u}$ and

\begin{equation}
	n(A) \defn \frac{A - \min A}{\max A - \min A}
\label{eq:norm}
\end{equation}

is a normalization of matrix $A$ into $[0,1]$. While Score-CAM does not need gradients, 
it requires as many forward passes through the network as the number of channels in the chosen layer,
 which is computationally expensive.

%--------------------------------------------------------------------------------------------------

\paragraph{Motivation}
\label{sec:motiv}

\iavr{Score-CAM considers each feature map as a mask in isolation. How about linear combinations?} 
Given a vector $\vw \in \real^{K_\ell}$ with $w_k$ its $k$-th element, let
\begin{equation}
	F(\vw) \defn f \left( \mathbf{u} \odot n \left( \operatorname{up} \left(
		\displaystyle\sum_k w_k A^k_\ell
	\right) \right) \right)_c.
\label{eq:s-obj}
\end{equation}
\ronan{If we assume that $\mathbf{u}_b = \vzero$ in~\eq{s-cam} and define $n(\vzero) \defn \vzero$ 
in~\eq{norm}, then we can rewrite the right-hand side of~\eq{s-cam} as
\begin{equation}
	\frac{F(\vw_0 + \delta \ve_k) - F(\vw_0)}{\delta},
\label{eq:s-cam2}
\end{equation}
where $\vw_0 = \vzero$, $\delta = 1$ and $\ve_k$ is the $k$-th standard basis vector of 
$\real^{K_\ell}$. This resembles the numerical approximation of the derivative $\pder{F}{w_k}(\vw_0)$,
 except that $\delta$ is not small as usual. One could compute derivatives efficiently by 
 standard backpropagation instead. It is then possible to iteratively optimize $F$ with respect
  to $\vw$, starting at any $\vw_0$.}

\iavr{As an alternative, consider masking-based methods relying on optimization in the input space, 
like \emph{meaningful perturbations} (MP) \parencite{fong2017interpretable} or 
\emph{extremal perturbations} \parencite{fong2019understanding}. In general, optimization takes the form
\begin{equation}
	S^c(\mathbf{u}) \defn \arg\max_{\vm \in \cM} f(\mathbf{u} \odot n(\operatorname{up}(\vm)))_c + \lambda R(\vm).
\label{eq:mask}
\end{equation}
Here, a mask $\vm$ is directly optimized and does not rely on feature maps, hence the saliency 
map $S^x(\mathbf{u})$ is not connected to any layer $\ell$. The mask is at the same or lower resolution 
than the input image. In the latter case, upsampling is still necessary.

In this approach, one indeed computes derivatives by backpropagation and indeed iteratively 
optimizes $\vm$. However, because $\vm$ is high-dimensional, there are constraints expressed by 
$\vm \in \cM$, \eg $\vm$ has a certain norm, and regularizers like $R(\vm)$, \eg $\vm$ is smooth in a 
certain way. This makes optimization harder or more expensive and introduces more hyperparameters 
like $\lambda$. One could simply constrain $\vm$ to lie in the linear span of $\{A_\ell^k\}_{k=1}
^{K_\ell}$ instead, like all CAM-based methods.}

\subsection*{Evaluating Interpretability}
\subsubsection*{Classification Metrics}
Average drop ($\AD$) and average increase ($\AI$) \autocite{chattopadhay2018grad} are 
well-established classification metrics. 
They measure the effect on the predicted class probabilities by masking the input image with the
 saliency map. Let $p^c_i$ and $o^c_i$ be the predicted probability for class $c$ given as input 
 the $i$-th test image $\vx_i$ and its masked version respectively. 
Masking refers to element-wise multiplication with the saliency map, which is at the same 
resolution as the original image with values in $[0,1]$. 
Let $N$ be the number of test images. Class $c$ is taken as the ground truth.

\emph{Average drop} ($\AD$) quantifies how much predictive power, measured as class probability, 
is lost when we only mask the image; lower is better:
\begin{equation}
	\AD(\%) \defn \frac{1}{N} \sum_{i=1}^N \frac{[p^c_i - o^c_i]_+}{p^c_i} \cdot 100.
\label{eq:ad}
\end{equation}

\emph{Average increase} ($\AI$), also known as \emph{increase in confidence}, measures the 
percentage of images where the masked image yields a higher class probability than the original; 
higher is better:
\begin{equation}
	\AI(\%) \defn \frac{1}{N} \sum_i^N \mathbb{1}_{p^c_i < o^c_i} \cdot 100.
\label{eq:ai}
\end{equation}

$\AD$ and $\AI$ are not defined in a symmetric way. $\AD$ measures changes in class probability 
whereas $\AI$ measures a percentage of images. It is possible that the percentage is high while 
the actual increase is small. Hence, it is possible that an attribution method improves both. 
Indeed, \autocite{poppi2021revisiting} observes that a trivial method called Fake-CAM outperforms 
state-of-the-art methods, including Score-CAM, by a large margin. Fake-CAM simply defines a 
saliency map where the top-left pixel is set to zero and is uniform elsewhere. 
This questions the purpose of $\AD$ and $\AI$.

%--------------------------------------------------------------------------------------------------
\subsubsection*{Localization metrics}
\label{sec:loc-metrics}
Several works measure the localization ability of saliency maps, using metrics from the 
\emph{weakly-supervised object localization} (WSOL) task.

We are given the saliency map $S^c$ obtained from test image $\vx$ for ground truth class $c$. 
We denote by $S^c_{\vp}$ its value at pixel $\vp$. We binarize the saliency map by thresholding at 
its average value and we take the bounding box of the largest connected component of the resulting 
mask as the predicted bounding box $B_p$, represented as a set of pixels. We compare this box 
against the set of ground truth bounding boxes $\cB$, which typically contains 1 or 2 boxes of the 
same class $c$, or with their union $U = \cup \cB$, again represented as a set of pixels. We also 
compare the predicted class label $c_p$ with the ground truth label $c$. All metrics take values in 
$[0,1]$ and are expressed as percentages, except SM~\eq{sm}, which is unbounded.

\emph{Official Metric (OM)}
measures the maximum overlap of the predicted bounding box with any ground truth bounding box, 
requiring that the predicted class label is correct:
\begin{equation}
	\OM \defn 1 - \paren{\max_{B \in \cB} \iou(B, B_p)} \mathbbm{1}_{c_p = c},
\label{eq:om}
\end{equation}
where $\iou$ is intersection over union.
% is defined as $\iou(B, B_p) \defn \frac{B \cap B_p}{B \cup B_p}$.

\emph{Localization Error (LE)} is similar but ignores the predicted class label:
\begin{equation}
	\LE \defn 1 - \max_{B \in \cB} \iou(B, B_p).
\label{eq:le}
\end{equation}

\emph{Pixel-wise $F_1$ score (F1)} is defined as $F_1 = 2 \frac{P R}{P + R}$, where 
\emph{precision} $P$ is the fraction of mass of the saliency map that is within the ground truth 
union
\begin{equation}
	P \defn \frac{\sum_{\vp \in U} S^c_{\vp}}{\sum_{\vp} S^c_{\vp}}
\label{eq:prec}
\end{equation}

and \emph{recall} $R$ is the fraction of the ground truth union that is covered by the saliency map
\begin{equation}
	R \defn \frac{\sum_{\vp \in U} S^c_{\vp}}{\card{U}}.
	\label{eq:rec}
\end{equation}

\emph{Box Accuracy (BA)\autocite{choe2020evaluating}} Given threshold values $\eta$ and $\delta$, 
we find the bounding box $B^\eta_p$ of the largest connected component of the binary mask 
$\set{\vp: S_{\vp} > \eta}$ and require that it overlaps by 
$\delta$ with at least one ground truth box:
\begin{equation}
	\BA(\eta, \delta) \defn \max_{B \in \cB} \mathbbm{1}_{\iou(B^\eta_p, B) \ge \delta}.
\label{eq:ba}
\end{equation}
After averaging over the test images, we take the maximum of this measure over a set of values 
$\eta$ and then the average over a set of values $\delta$.\\
%--------------------------------------------------------------------------------------------------
\emph{Standard Pointing game (SP)\autocite{zhang2018top}} We find the pixel 
$\vp^* \defn \arg\max_{\vp} S^c_{\vp}$ having the maximum saliency value and 
require that it lands in any of the ground truth bounding boxes:
\begin{equation}
	\spg \defn \mathbbm{1}_{\vp^* \in U}.
\label{eq:spg}
\end{equation}\\

\emph{Energy Pointing game (EP)\autocite{wang2020score}} is equivalent to precision~\eq{prec}.\\

\emph{Saliency Metric (SM)\autocite{dabkowski2017real}} penalizes the size of the predicted bounding
 box $B_p$ relative to the image and the cross-entropy
 loss:
\begin{equation}
	\SM \defn \log \max\paren{ 0.05, \frac{\card{B_p}}{hw} } - \log p^c,
\label{eq:sm}
\end{equation}
where $h \times w$ is the input image resolution and $p^c$ is the precicted probability for ground 
truth class label $c$.