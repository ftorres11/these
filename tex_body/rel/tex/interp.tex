%--------------------------------------------------------------------------------------------------
\section{Interpretability}
\label{rel:sec_int}
As deep learning based models for Computer Vision have continued to improve in their recognition 
properties, their structure and functioning have become more opaque; in turn making these 
technologies seen as black boxes. A black-box model is defined as a model for which its 
interpretation is not straightforward for humans \autocite{petch2022opening}. In recent years 
with the assimilation of deeplearning into everyday tasks, and the implicit effect these models 
are having on human lives; the novel research field of \emph{Interpretability} has been brought 
forth to open up this black-box behaviour. This research field investigated alongside two 
directions(\cite{mythos_interp}, \cite{guidotti2018survey}, \cite{zhang2021survey}):

\begin{enumerate}
	\item \emph{Post-hoc interpretability} considers the model as a black-box and provides 
	explanations based on inputs and outputs, without modifying the model or its training process.
	\item \emph{Transparency} modifies the model or the training process to better explain the 
	behavior of the inner parts of the model.
\end{enumerate}

\subsection{Transparency}
\label{rel:sub_transp}
Transparent 
%Transparency in interpretability research represents a pivotal pursuit within the broader 
%landscape of artificial intelligence. In the ever-evolving field of machine learning, 
%particularly deep neural networks, the inherent complexity of models often gives rise to a lack of 
%transparency, making it challenging to comprehend the decision-making processes. Researchers 
%focusing on transparency in interpretability seek to address this opacity by developing 
%methodologies and frameworks that not only reveal how models arrive at specific outcomes but 
%also provide a comprehensive understanding of the underlying mechanisms. The ultimate goal is to 
%empower users, stakeholders, and policymakers with the knowledge necessary to trust, validate, and 
%navigate the applications of artificial intelligence effectively. This emphasis on transparency not 
%only enhances the accountability of AI systems but also contributes to broader societal acceptance 
%and ethical deployment of these powerful technologies.
Approaches are grouped in a number of categories according to the type of the given explanation. \\
\emph{Rule-based methods} (\cite{wu2018beyond}, \cite{wu2020regional}) train a decision tree as a 
surrogate regularization term to force a network to be easily approximated by a decision tree.
% or learn a set of logit rules as an explanation~\citep{azzolin2022global}.
\emph{Hidden semantics-based methods} (\cite{bau2017network}, \cite{zhou2018interpreting}, 
\cite{zhang2018interpretable}, \cite{zhou2014object},\cite{bohle2022b}. \cite{bohle2024b}) aim to 
make a convolutional network learn disentangled hidden semantics with hierarchical structure or 
object-level concepts.\\
\emph{Prototype-based methods} (\cite{li2018deep}, \cite{chen2019looks}) learn a set of prototypes 
or parts as an intermediate representation in the network, which can be aligned with categories. 
\emph{Attribution-based methods} (\cite{ismail2021improving}, \cite{Zhou_2022_BMVC}, \\
\cite{ross2017right}, \cite{ghaeini2019saliency}) usually modify the architecture of a network or 
the training process to help post-hoc methods produce better saliency maps. Unlike 
(\cite{ross2017right}, \cite{ghaeini2019saliency}), saliency guided localization 
\autocite{Zhou_2022_BMVC} does not need ground truth explanations but replaces them with 
information bottleneck attribution \autocite{schulz2020restricting}. Finally, saliency-guided 
training \autocite{ismail2021improving} minimizes the KL divergence between the output of original 
and masked images.

\subsection{Post-Hoc Interpretability}
\label{rel:sub_post}

Approaches can be grouped into a number of possibly overlapping categories. 
\emph{Gradient-based methods} (\cite{adebayo2018local}, \cite{guidedbackprop}, 
\cite{baehrens2010explain}, \cite{simonyan2013deep}, \cite{smilkov2017smoothgrad}, 
\cite{bach2015pixel}, \cite{sundararajan2017axiomatic}) use the gradient of a target class score 
with respect to the input to compute the contribution of different input regions to the prediction. 
\emph{CAM-based methods} (\cite{wang2020score}, 
\cite{chattopadhay2018grad}, \cite{selvaraju2017grad}, 
\cite{axiombased}, \cite{jiang2021layercam}, \cite{ablationcam}) compute saliency maps 
as a linear combination of feature maps, with different definitions for the weights. 

\emph{Occlusion or masking-based methods} (\cite{petsiuk2018rise}, \cite{fong2017interpretable}, 
\cite{fong2019understanding}, \cite{schulz2020restricting}, \cite{ribeiro2016should}) apply a 
number of candidate masks in the input space, measure their effect on the prediction and then 
combine them into a saliency map. Masking in feature space has been explored too 
\autocite{schulz2020restricting}. 

Finally, \emph{learning-based methods} (\cite{chang2018explaining}, \cite{dabkowski2017real}, 
\cite{phang2020investigating}, \cite{zolna2020classifier}, \cite{schulz2020restricting}) learn an 
additional network or branch on extra data to produce an explanation map for a given input. 

\paragraph{Notation}
\label{sec:oc_notation}

Consider a classifier network $:fx \cX \to \real^C$ that maps an input image $\mathbf{u} \in \cX$ to a 
logit vector $\vy = f(\mathbf{u}) \in \real^C$, where $\cX$ is the image space and $C$ is the number 
of classes. We denote by $y_c = f(\mathbf{u})_c$ the predicted logit and by $p_c = \softmax(\vy)_c 
\defn e^{y_c} / \sum_j e^{y_j}$ the predicted probability for class $c$. For layer $\ell$ 
with $K_\ell$ channels, we denote by $A^k_\ell = f^k_\ell(\mathbf{u}) \in \real^{h_\ell \times w_\ell}$ 
the feature map for channel $k \in \{1,\dots,K_\ell\}$, with spatial resolution $h_\ell \times 
w_\ell$. Because of $\relu$ non-linearities, we assume that feature maps are non-negative. 
Similarly, we denote by $S_\ell \in \real^{h_\ell \times w_\ell}$ a 2D saliency map.

%--------------------------------------------------------------------------------------------------

\paragraph{Background: CAM-based saliency maps}
\label{sec:oc_back}

Given a layer $\ell$ and a class of interest $c$, we consider saliency maps given by the general 
formula
\begin{equation}
	S^c_\ell(\mathbf{u}) \defn h \left( \sum_k w^c_k A^k_\ell \right),
\label{eq:sal}
\end{equation}
where $w^c_k$ are weights defining a linear combination over channels and $h$ is an activation 
function. CAM \parencite{zhou2016learning} is defined for the last layer $L$ only with $h$ being the 
identity mapping and $w^c_k$ being the classifier weight connecting the $k$-th channel with 
class $c$. Grad-CAM \parencite{selvaraju2017grad} is defined for any layer $\ell$ with $h = \relu$ and 
weights
\begin{equation}
	w^c_k \defn \gap \left( \pder{y_c}{A^k_\ell} \right),
\label{eq:gcam}
\end{equation}
where $\gap$ is global average pooling.
% and $\softmax(\vy)_c = e^{y_c} / \sum_j e^{y_j}$ is the predicted probability of class $c$.
The motivation for $\relu$ is that we are only interested in features that have a positive effect 
on the class of interest, \ie pixels whose intensity should be increased in order to increase $y_c$.

Score-CAM \parencite{wang2020score} is also defined for any layer $\ell$ with $h = \relu$ and weights 
$w^c_k \defn \softmax(\mathbf{u}^c)_k$.  Softmax normalization considers positive channel contributions 
only and attends to few feature maps.
%that \alert{produce less highlighted areas in the saliency map}. \iavr{Last part unclear.}
Here, vector $\mathbf{u}^c \in \real^{K_\ell}$ measures the increase in confidence for class $c$ that 
compares a known baseline image $\mathbf{u}_b$ with the input image $\mathbf{u}$ masked according to feature 
map $A^k_\ell$, for all channels $k$:

\begin{equation}
	u^c_k \defn f(\mathbf{u} \odot n(\operatorname{up}( A^k_\ell )))_c - f(\mathbf{u}_b)_c,
\label{eq:s-cam}
\end{equation}

where $\odot$ is the Hadamard product. For this to work, the feature map $A^k_\ell$ is adapted
 to $\mathbf{u}$ first$:\operatorname{up}$ denotes upsampling to the spatial resolution of $\mathbf{u}$ and

\begin{equation}
	n(A) \defn \frac{A - \min A}{\max A - \min A}
\label{eq:norm}
\end{equation}

is a normalization of matrix $A$ into $[0,1]$. While Score-CAM does not need gradients, 
it requires as many forward passes through the network as the number of channels in the chosen layer,
 which is computationally expensive.

%--------------------------------------------------------------------------------------------------

\paragraph{Motivation}
\label{sec:motiv}

\iavr{Score-CAM considers each feature map as a mask in isolation. How about linear combinations?} 
Given a vector $\vw \in \real^{K_\ell}$ with $w_k$ its $k$-th element, let
\begin{equation}
	F(\vw) \defn f \left( \mathbf{u} \odot n \left( \operatorname{up} \left(
		\displaystyle\sum_k w_k A^k_\ell
	\right) \right) \right)_c.
\label{eq:s-obj}
\end{equation}
\ronan{If we assume that $\mathbf{u}_b = \vzero$ in~\eq{s-cam} and define $n(\vzero) \defn \vzero$ 
in~\eq{norm}, then we can rewrite the right-hand side of~\eq{s-cam} as
\begin{equation}
	\frac{F(\vw_0 + \delta \ve_k) - F(\vw_0)}{\delta},
\label{eq:s-cam2}
\end{equation}
where $\vw_0 = \vzero$, $\delta = 1$ and $\ve_k$ is the $k$-th standard basis vector of 
$\real^{K_\ell}$. This resembles the numerical approximation of the derivative $\pder{F}{w_k}(\vw_0)$,
 except that $\delta$ is not small as usual. One could compute derivatives efficiently by 
 standard backpropagation instead. It is then possible to iteratively optimize $F$ with respect
  to $\vw$, starting at any $\vw_0$.}

\iavr{As an alternative, consider masking-based methods relying on optimization in the input space, 
like \emph{meaningful perturbations} (MP) \parencite{fong2017interpretable} or 
\emph{extremal perturbations} \parencite{fong2019understanding}. In general, optimization takes the form
\begin{equation}
	S^c(\mathbf{u}) \defn \arg\max_{\vm \in \cM} f(\mathbf{u} \odot n(\operatorname{up}(\vm)))_c + \lambda R(\vm).
\label{eq:mask}
\end{equation}
Here, a mask $\vm$ is directly optimized and does not rely on feature maps, hence the saliency 
map $S^x(\mathbf{u})$ is not connected to any layer $\ell$. The mask is at the same or lower resolution 
than the input image. In the latter case, upsampling is still necessary.

In this approach, one indeed computes derivatives by backpropagation and indeed iteratively 
optimizes $\vm$. However, because $\vm$ is high-dimensional, there are constraints expressed by 
$\vm \in \cM$, \eg $\vm$ has a certain norm, and regularizers like $R(\vm)$, \eg $\vm$ is smooth in a 
certain way. This makes optimization harder or more expensive and introduces more hyperparameters 
like $\lambda$. One could simply constrain $\vm$ to lie in the linear span of $\{A_\ell^k\}_{k=1}
^{K_\ell}$ instead, like all CAM-based methods.}

\subsection{Discussion}
