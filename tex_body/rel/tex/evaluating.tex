%--------------------------------------------------------------------------------------------------
\subsection{Evaluating Interpretability}
\label{rel:sub_evl}
Following the \emph{desiredata of model interpretabiltiy}, proposing interpretations 
does not compose the only step in understanding the prediction process of a model; in order to 
claim to be interpretable, the effect of its explanations should be quantifiable for an approach 
or model. Lipton addresses this in the discussion section of \emph{The Mythos of Model 
Interpretability}, where he points: 
\begin{quote}
	\emph{To be meaningful, any assertion regarding interpretability should fix a specific 
	definition. If the model satiesfies a form of transparency, this can be shown directly. 
	For post-hoc interpretability, papers ought to fix a clear objective and demonstrate evidence 
	that the offered form of interpretation achieves it.}
\end{quote}
On one hand, this definition is not completely up-to-date for deep image recognition 
models. In particular, given the difficulties of the original definition of transparency covering 
deep models, especially with the introduction of active interpretability approaches introduced 
previously. A direct display of transparency is not straightforward, particularly given that some 
of these proposals involve modifying the behaviour of models or lack inherent interpretability. 
Conversely, post-hoc interpretations are frequently found in the input space. As a result, 
the impact of these modifications is traditionally evaluated through human centric assessments of 
\emph{trust} or \emph{reliablity}. In the other hand, a quantitative approach, involves probing changes 
in the predictive power of the input masked by the saliency map (\emph{interpretable object 
recognition}), randomly perturbing the input guided by saliency information described by the 
attribution map (\emph{causal analysis}), and considering these attributions in a weakly supervised 
object localization manner (\emph{interpretable object localization}). We present a brief summary 
of these metrics in \autoref{tab:interp_mets}
\input{tab/rel/tex/interp_metrics.tex}

%--------------------------------------------------------------------------------------------------
\subsubsection{Trust and Reliability} Following the desiredata of model interpretability. In this, 
the interpretabiltiy properties of \emph{trust} and \emph{informativeness} are the main driving 
force of this approach. To begin with, it is desired to be able to trust a model on its predictions, 
the examples for which it is right and how often is it; this in turn can be seen via 
informativeness. Given the black-box behaviour of image recognition models, informativeness can be 
gained by taking interest in salient regions of an image leading to a prediction. Therefore, 
assessing human trust of a saliency map describing a prediction is traditionally used to assess 
interpretability (\cite{ribeiro2016should}, \cite{zhou2016learning}, \cite{selvaraju2017grad}, 
\cite{chattopadhay2018grad}, \cite{bau2017network}). Nevertheless, this evaluation methodology is 
not favoured in recent times, as human interpretations are not always aligned with what the model 
or classifier deems important; conversely, this kind of evaluation can be often biased or not 
replicable at all.

% -------------------------------------------------------------------------------------------------
\subsubsection{Interpretable Object Recognition} 
\label{sec:class_metrics}
Proposed based on the capability of current image 
recognition models to provide outputs over the distribution of classes that it was trained in, 
for any given input. Conversely, since saliency maps are class-specific, a logical manner of 
assessing their interpretability properties involves masking on the input space and measuring the 
changes of the predictive power of the model with regards of the highlighted regions provided by 
the saliency map. This methodology is first proposed by \cite{chattopadhay2018grad}, where in 
addition of traditional human evaluation of saliency maps, their predictive capabilities are 
measured using \glsfirst{ad}, \glsfirst{ai}.\\

\paragraph{Notation}
Let $p^c_i$ and $o^c_i$ be the predicted probability for class $c$ given as input  the $i$-th test 
image $\mathbf{u}$ and its masked version respectively. Masking refers to element-wise 
multiplication with the saliency map, which is at the same resolution as the original image with 
values in $[0,1]$. Let $N$ be the number of test images. Class $c$ is taken as the ground truth.\\

\noindent \emph{AD} quantifies how much predictive power, measured as class probability, 
is lost when we only mask the image; lower is better:
\begin{equation}
	\AD \defn \frac{1}{N} \sum_{i=1}^N \frac{[p^c_i - o^c_i]_+}{p^c_i} \cdot 100.
\label{eq:ad}
\end{equation}

\noindent \emph{AI}, also known as \emph{increase in confidence}, measures the 
percentage of images where the masked image yields a higher class probability than the original; 
higher is better:
\begin{equation}
	\AI \defn \frac{1}{N} \sum_i^N \mathbb{1}_{p^c_i < o^c_i} \cdot 100.
\label{eq:ai}
\end{equation}

$\AD$ and $\AI$ are not defined symmetrically. $\AD$ measures changes in class probability 
whereas $\AI$ measures a percentage of images. It is possible that the percentage is high while 
the actual increase is small. Hence, it is possible that an attribution method improves both. 
Indeed, \autocite{poppi2021revisiting} observes that a trivial method called Fake-CAM outperforms 
state-of-the-art methods, including Score-CAM, by a large margin. Fake-CAM simply defines a 
saliency map where the top-left pixel is set to zero and is uniform elsewhere. 
This questions the purpose of $\AD$ and $\AI$. Addressing this, Poppi proposes \gls{adcc}, a metric 
that combines AD and two complementary measurements: \emph{Coherency} and \emph{Complexity}.
In detail:\\

\noindent \emph{Coherency} answers the requirement that the CAM-attribution of one image has to 
equate to the corresponding attribution of the explanation map obtained by the combination of said 
attribution map and the original input image. This is defined using the Pearson Correlation 
Coefficient between both attributions following:

\begin{equation}
	\mbox{Coherency}(u) = \frac{\mbox{Cov}\left(\mbox{CAM}_c(\mathbf{u}\odot \mbox{CAM}_c(\mathbf{u})),
	                       \mbox{CAM}_c(\mathbf{u})\right)}{\sigma\mbox{CAM}_c(\mathbf{u}
						   \odot \mbox{CAM}_c(\mathbf{u}))\sigma\mbox{CAM}_c(\mathbf{u})}
\end{equation}

\noindent\emph{Complexity} is addressed to the requirement of CAM attributions being as 
\emph{simple} or less complex as possible. It is defined using the $L_1$ norm as a proxy of 
complexity:

\begin{equation}
	\mbox{Complexity}(\mathbf{u}) = ||\mbox{CAM}_c(\mathbf{u})||_1
\end{equation}

\noindent \emph{ADCC} takes into consideration the previously introduced metrics and 
\gls{ad}, encompassing these metrics taking their harmonic mean in the following manner:

\begin{equation}
	\mbox{ADCC}(\mathbf{u}) = 3\left(\frac{1}{\mbox{Coherency}(\mathbf{u})} + 
	                           \frac{1}{1-\mbox{Complexity}(\mathbf{u}) + 
							   \frac{1}{1-AD(\mathbf{u})}}\right)^{-1}
\end{equation}
% -------------------------------------------------------------------------------------------------
\subsubsection{Causal Analysis} 
\label{sec:causal_metrics}
Inspired by the work described in (\cite{fong2017interpretable}, \cite{fong2019understanding}), 
instead adding perturbations on an image level in order to generate 
and attribution map, Vitali proposes an evaluation scheme where images are modified 
according to their saliency maps, and the subsequent change of predictive power is measured. This 
methodology contemplates two styles of modifications: on one hand, the input image is blurred 
entirely, and the information is reconstructed by the \glsfirst{ins} of pixels following the 
saliency map; conversely, information can be steadily removed from the image by iterative
\glsfirst{del} of salient pixels \autocite{petsiuk2018rise}.\\

\paragraph{Notation} let $x$ be an input image with the corresponding saliency map $s^c$ and $N$ the 
number of pixels removed per step, we calculate the predicted probability $p^c_n$ at step $n$ for 
groundtruth class $c$ using the model $f$.  Similar to \gls{ad} and \gls{ai} Vitali 
considers $s^c$ to maintain the same image resolution as $x$ and be normalized in values $[0,1]$.\\

\noindent \emph{Insertion} measures the increase of probability when introducing pixels from an 
input image into a blurry version of itself, following the salient order described by the saliency 
map. Best described in \autoref{algo:rel_ins}\\
\input{tex_body/rel/tex/ins.tex}
\noindent \emph{Deletion} measures the decrease of probability as pixels are zeroed out in an input 
image, following the salient order described by the saliency map. Best 
described in \autoref{algo:rel_del}\\
\input{tex_body/rel/tex/del.tex}
%--------------------------------------------------------------------------------------------------
\subsubsection{Interpretable object localization}
\label{sec:loc-metrics}
Following in line with the desire of saliency maps aligning with the object belonging to the class 
they are computed for, interpretability properties are evaluated using object localization.
One approach considering this, is found within the \emph{Broden} dataset 
\autocite{bau2017network}. In this work, interpretability is measured at the unit level, where 
individual channels from a convolutional layer are compared with semantic concepts previously 
annotated within the dataset. 

\noindent In addition to the evaluation procedure proposed in Broden, weakly supervised evaluation 
approaches are taken into consideration for this task. 

%--------------------------------------------------------------------------------------------------
\paragraph{Notation} Given the saliency map $S^c$ obtained from test image $\vx$ for ground truth 
class $c$. We denote by $S^c_{\vp}$ its value at pixel $\vp$. We binarize the saliency map by 
thresholding at its average value, and we take the bounding box of the largest connected component 
of the resulting mask as the predicted bounding box $B_p$, represented as a set of pixels. This 
box is compared against the set of ground truth bounding boxes $\cB$, which typically contains 1 
or 2 boxes of the same class $c$, or with their union $U = \cup \cB$, again represented as a set 
of pixels. We also compare the predicted class label $c_p$ with the ground truth label $c$. All 
metrics take values in $[0,1]$ and are expressed as percentages, except SM~\eq{sm}, which is 
unbounded.\\

\noindent \emph{Unit Interpretability} Defined in Broden, this metric is different from those 
defined in the coming weakly-supervised approaches; compares each individual feature map $A^k$ 
with the localization mask $L_c$ of class $c$ for a given image $x$. To compute this comparison, 
the activation map $A^k$ is upsampled into the original image resolution and then binarized 
generating the binary mask $M^k$. This mask is used following:

\begin{equation}
	\iou_{k,c} \defn \frac{\sum \abs{M^k(\mathbf{u}) \cap L_c(x)}}
					      {\sum \abs{M^k(\mathbf{u}) \cup L_c(x)}}
\end{equation}

\noindent \emph{Official Metric (OM)} measures the maximum overlap of the predicted bounding box with any 
ground truth bounding box, requiring that the predicted class label is correct:
\begin{equation}
	\OM \defn 1 - \paren{\max_{B \in \cB} \iou(B, B_p)} \mathbbm{1}_{c_p = c},
\label{eq:om}
\end{equation}
where $\iou$ is intersection over union.\\
% is defined as $\iou(B, B_p) \defn \frac{B \cap B_p}{B \cup B_p}$.

\noindent \emph{Localization Error (LE)} is similar but ignores the predicted class label:
\begin{equation}
	\LE \defn 1 - \max_{B \in \cB} \iou(B, B_p).
\label{eq:le}
\end{equation}

\noindent \emph{Pixel-wise $F_1$ score (F1)} is defined as $F_1 = 2 \frac{P R}{P + R}$, where 
\emph{precision} $P$ is the fraction of mass of the saliency map that is within the ground truth 
union
\begin{equation}
	P \defn \frac{\sum_{\vp \in U} S^c_{\vp}}{\sum_{\vp} S^c_{\vp}}
\label{eq:prec}
\end{equation}

and \emph{recall} $R$ is the fraction of the ground truth union that is covered by the saliency map
\begin{equation}
	R \defn \frac{\sum_{\vp \in U} S^c_{\vp}}{\card{U}}.
	\label{eq:rec}
\end{equation}

\noindent \emph{Box Accuracy (BA) \autocite{choe2020evaluating}} Given threshold values $\eta$ and $\delta$, 
we find the bounding box $B^\eta_p$ of the largest connected component of the binary mask 
$\set{\vp: S_{\vp} > \eta}$ and require that it overlaps by 
$\delta$ with at least one ground truth box:
\begin{equation}
	\BA(\eta, \delta) \defn \max_{B \in \cB} \mathbbm{1}_{\iou(B^\eta_p, B) \ge \delta}.
\label{eq:ba}
\end{equation}
After averaging over the test images, we take the maximum of this measure over a set of values 
$\eta$ and then the average over a set of values $\delta$.\\

%--------------------------------------------------------------------------------------------------
\noindent \emph{Standard Pointing game (SP)\autocite{zhang2018top}} We find the pixel 
$\vp^* \defn \arg\max_{\vp} S^c_{\vp}$ having the maximum saliency value and 
require that it lands in any of the ground truth bounding boxes:
\begin{equation}
	\spg \defn \mathbbm{1}_{\vp^* \in U}.
\label{eq:spg}
\end{equation}\\

\noindent \emph{Energy Pointing game (EP) \autocite{wang2020score}} is equivalent to precision~\eq{prec}.\\

\noindent \emph{Saliency Metric (SM) \autocite{dabkowski2017real}} penalizes the size of the predicted bounding 
box $B_p$ relative to the image and the cross-entropy loss:
\begin{equation}
	\SM \defn \log \max\paren{ 0.05, \frac{\card{B_p}}{hw} } - \log p^c,
\label{eq:sm}
\end{equation}
where $h \times w$ is the input image resolution and $p^c$ is the predicted probability for ground 
truth class label $c$.