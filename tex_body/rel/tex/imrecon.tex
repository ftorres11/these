%--------------------------------------------------------------------------------------------------
\section{Image Recognition Models}
\label{rel:sec_imrecon}
\subsection{Background}
\label{rel:sub_background}
%\addcontentsline{toc}{section}{Image Recognition Models}

\subsection{Convolutional Neural Networks}
\label{rel:sub_cnn}
\cite{fukushima1975cognitron}, LeNet \cite{lecun1998gradient}, AlexNet \cite{krizhevsky2012imagenet}, VGG \cite{simonyan2015deep}, ResNet \cite{he2016deep}, 
Inception (\cite{szegedy2015going}, \cite{szegedy2016rethinking}), NasNet (\cite{zoph2018learning})
EfficientNet(\cite{tan2019efficientnet})

ResNet Strikes back (\cite{wightman2021resnet})
%\addcontentsline{toc}{subsection}{Convolutional Neural Networks}

\subsection{Attention-Based Architectures}
\label{rel:sub_att}
%\addcontentsline{toc}{subsection}{Attention-Based Architectures}
Attention is a powerful mechanism that has been introduced into convolutional networks in several 
ocasions (\cite{bello2019attention}, \cite{ramachandran2019stand}, \cite{shen2020global}). 
With the success of vision transformers (ViT) (\cite{dosovitskiy2020image}), fully attention-based 
architectures are now competitive with convolutional networks. To benefit from both self-attention 
and convolutional layers, some hybrid architectures employ convolutional layers before the vision transformer 
. Others, such as Swin (\cite{liu2021swin}) and PiT 
(\cite{heo2021rethinking}), introduces a pyramid structure to share local spatial information 
while reducing the spatial resolution, as in convolutional networks. 

SCOUTER (\cite{li2021scouter}) uses slot attention (\cite{locatello2020object}) to build a class 
specific pooling mechanism, which does not scale well to more than 20 classes. 

\subsection{Hybrid Architectures}
\label{rel:sub_hybrid}
(\cite{graham2021levit}, \cite{xiao2021early})
Conformer (\cite{peng2021conformer}) proposes a dual network structure to retrain and fuse local 
convolutional features with global representations. Our method merely provides a simple 
attention-based pooling mechanism inspired by transformers that can work with any architecture, 
even pretrained and frozen. PatchConvNet (\cite{touvron2021augmenting}) replaces global average pooling by an 
attention-based pooling layer.
