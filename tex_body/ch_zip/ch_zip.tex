\chapter{Zero Information in Interpretability}
\chaptertoc{}
%    Deep neural network predictions are often explained by means of class specific saliency maps, highlighting parts of the input responsible for the prediction. Different methods and evaluation metrics are based on masking inputs by such saliency maps. Transformer architectures have a built-in attention mechanism and their pooling is based on a class agnostic raw attention map. But, how exactly are these two concepts connected and can this connection be exploited to improve the interpretability properties of a network?

%In this work, we observe that the raw attention map has the same form as a class agnostic saliency map and that attention-based pooling is a form of masking in the feature space. Motivated by this observation, we design an attention-based pooling mechanism as a replacement of global average pooling in convolutional networks and we study  its effect in interpreting the network predictions. This mechanism, called \emph{CA-Stream}}, takes the form of a  stream composed of cross attention blocks that interact with features at different network stages. The stream can be  trained while the network remains frozen. We show that our approach improves the interpretability properties of  different networks, while maintaining classification accuracy.}

\section{Articles}
    \fullcite{}
