\section{Introduction}
\label{sec:intro}

%Recent advances in Machine Learning, and particularly the introduction and development of Deep Learning models, have led the field of AI to experience a remarkable growth. Since the development of the first Convolutional Neural Networks (CNNs) \citep{10.1162/neco.1989.1.4.541}, more complicated architectures emerged, with a very large number of parameters \citep{he2015deep, 10.1145/3065386, simonyan2015deep, szegedy2014going}. Among these, the ResNeXt \citep{liu2022convnet} architecture stands out as one of the latest achievements, demonstrating  comparable accuracy to state-of-the-art models in Computer Vision. However, this heightened complexity, contributing to enhanced accuracy scores, came at the price of decreased model transparency and explainability. Those models were soon labeled as ``black boxes". Consequently, the field of Explainable Artificial Intelligence (XAI) has emerged, trying to demystify a model's functioning by attributing the model's decision to the input features. Although XAI has yielded noteworthy breakthroughs, it is still in a phase of ongoing development.

Classical methods in the field of XAI construct an attribution map for each input, which calculates the importance of individual features \citep{sundararajan2017axiomatic, selvaraju2017gradcam, Chattopadhay_2018, Wang_2020_CVPR_Workshops, lundberg2017unified, lrp}. While different methodologies have emerged \citep{BARREDOARRIETA202082}, several promising approaches rely on feature occlusion, assessing the importance of a specific input component by masking its information and quantifying the resulting decline in the model's prediction score \citep{9093360, ribeiro2016why, lundberg2017unified, jung2021better, novello2022making, dabkowski2017real}. Certain methods extend this occlusion to cover the entire input, hence defining a ``baseline" point \footnote{Also defined as ``reference point" \citep{shrikumar2017just, shrikumar2019learning}  or ``root point" \citep{MONTAVON2017211}} which is considered to be devoid of any information. In order to occlude information contained in a subset of features, researchers in Computer Vision adopted different heuristic techniques. These techniques encompass subtle perturbations, such as introducing minor random noise—an approach that has given rise to perturbation methods \citep{hsieh2021evaluations}-, or applied more intensive interventions like blackening or blurring the concealed image parts, substituting them with random noise, or introducing maximum distance values based on predefined pixels \citep{haug2021baselines, sturmfels2020visualizing}. These assorted heuristic strategies have been systematically categorized \citep{haug2021baselines} and applied across different attribution methods \citep{sturmfels2020visualizing, kindermans2017unreliability}. Nonetheless, researchers have noted a lack of consistency in the outcomes produced by each method, with none demonstrating a decisive advantage over others. 

Concerns swiftly emerged regarding the validity of such heuristic filling techniques \citep{hsieh2021evaluations, kindermans2017unreliability}, while perturbation methods failed to yield notable results. These methods prompted critical issues that captured researchers' attention, particularly the challenge of introducing bias to the model's decision [TODO] and the problem of Out-of-Distribution data (OOD) \citep{gomez2022metrics, hase2021outofdistribution, qiu2021resisting}. The first issue involves the potential introduction of bias when replacing one value with another, subsequently affecting the model's decision-making process. When the number of masked features increases, this bias can escalate, substantially influencing the model's outcomes. The second issue revolves around the construction of images that lie outside the data distribution in which the model was trained on. A problem which is referred as ``Out of distribution" (OOD) problem in bibliography \citep{qiu2021resisting, janzing20a}. The filled images could feature regions obscured by blurring, blackening, or random noise—elements not encountered during the model's training. As a consequence, a change in the model's decision might be attributed to its incapability to effectively assess such uncharted regions within the data distribution. While various methods have attempted to mitigate the challenge of OOD data by introducing new filling techniques \citep{jethani2021learned, yoon2018invase, chen2018learning}, to the best of our knowledge, none of the methods developed thus far have succeeded in simultaneously addressing both problems. The pursuit of a robust technique to effectively obscure information within an image remains an ongoing challenge.

Our focus lies in highlighting the issue of concealing information within images and scrutinizing the repercussions of an indiscriminate filling approach. We contend that a robust filling method should obscure image segments in a manner that the concealed parts impart no information to the model, while the visible segments maintain an equivalent contribution to the model's decision. We translate those criteria into a loss function and employ an optimization algorithm that simultaneously adheres to both these principles, all while respecting the data distribution. \newline

\par \noindent
In summary, we make the following contributions:
\begin{enumerate}[itemsep=2pt, parsep=0pt, topsep=3pt]
	\item We further examine the problem of introducing bias when hiding subsets of features and when effectively demonstrate how it can perturb a model's decision (\autoref{sec:problem}).
	\item We introduce the ZIP method, a novel approach that is designed to optimally hide information from the data, for the reconstruction of unbiased and in-distribution data (\autoref{sec:method}). 
	\item We propose new metrics that serve to quantify the impact of the Out-of-Distribution (OOD) criterion and the inherent bias problem, expounded in (\autoref{sec:metrics}).
	\item We introduce revised and resilient versions of different evaluation metrics that are based on the ZIP algorithm. We then conduct a reevaluation of various attribution methods, gauging their performance through these newly defined criteria (\autoref{sec:results}).
\end{enumerate}

