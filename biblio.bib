% Grad-CAM++
@inproceedings{chattopadhay2018grad,
    author = {A. Chattopadhay and A. Sarkar and P. Howlader and V. N. Balasubramanian},
    booktitle = {WACV},
    title = {Grad-{CAM}++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks},
    year = {2018},
    volume = {},
    issn = {},
    keywords = {visualization;heating systems;neurons;machine learning;predictive models;mathematical model},
}

% WSOL Evaluation
@inproceedings{choe2020evaluating,
    title = {Evaluating Weakly Supervised Object Localization Methods Right},
    author={Choe, Junsuk and Oh, Seong Joon and Lee, Seungho and Chun, Sanghyuk and Akata, Zeynep and Shim, Hyunjung},
    booktitle = {CVPR},
    year = {2020}
} 

% Top-Down Neural Attention
@article{zhang2018top,
    title={Top-Down Neural Attention by Excitation Backprop},
    volume={126},
    journal={IJCV},
    author={Zhang, Jianming and Bargal, Sarah Adel and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
    year={2017},
    pages={1084-1102}
}

% Score-CAM 
@inproceedings{wang2020score,
    author = {Wang, Haofan and Wang, Zifan and Du, Mengnan and Yang, Fan and Zhang, Zijian and Ding, Sirui and Mardziel, Piotr and Hu, Xia},
    title = {Score-{CAM}: Score-Weighted Visual Explanations for Convolutional Neural Networks},
    booktitle = {CVPR Workshop},
    year = {2020}
} 

% Real Time Saliency for BB Classifiers
@article{dabkowski2017real,
    author = {Dabkowski, Piotr and Gal, Yarin},
    journal = {NIPS},
    editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    publisher = {Curran Associates, Inc.},
    title = {Real Time Image Saliency for Black Box Classifiers},
    year = {2017}
}

% ADAM optimizer
@article{kingma2014adam,
    author    = {Diederik P. Kingma and
                Jimmy Ba},
    editor    = {Yoshua Bengio and
                Yann LeCun},
    title     = {Adam: {A} Method for Stochastic Optimization},
    journal = {ICLR},
    year      = {2015},
}

% RISE, Ins-Del
@article{petsiuk2018rise,
  title = {RISE: Randomized Input Sampling for Explanation of Black-box Models},
  author = {Vitali Petsiuk and Abir Das and Kate Saenko},
  journal = {BMVC},
  year = {2018}
}

% Perturbation
@inproceedings{fong2017interpretable,
    author = {Fong, Ruth C. and Vedaldi, Andrea},
    title = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
    booktitle = {ICCV},
    year = {2017}
} 

% Ex-Perturbation
@inproceedings{fong2019understanding,
  title={Understanding deep networks via extremal perturbations and smooth masks},
  author={Fong, Ruth and Patrick, Mandela and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={2950--2958},
  year={2019}
}

% Flow-Bottleneck restriction
@article{schulz2020restricting,
  title={Restricting the flow: Information bottlenecks for attribution},
  author={Schulz, Karl and Sixt, Leon and Tombari, Federico and Landgraf, Tim},
  journal={arXiv preprint arXiv:2001.00396},
  year={2020}
}

% LIME
@inproceedings{ribeiro2016should,
    author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
    title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
    year = {2016},
    isbn = {9781450342322},
    booktitle = {SIGKDD},
    keywords = {interpretability, black box classifier, explaining machine learning, interpretable machine learning},
    location = {San Francisco, California, USA},
    series = {KDD '16}
}

% Opti-CAM
@article{zhang2023opti,
  title={Opti-CAM: Optimizing saliency maps for interpretability},
  author={Zhang, Hanwei and Torres, Felipe and Sicre, Ronan and Avrithis, Yannis and Ayache, Stephane},
  journal={arXiv preprint arXiv:2301.07002},
  year={2023}
}

% Transformer + CNN
@inproceedings{bello2019attention,
  title={Attention augmented convolutional networks},
  author={Bello, Irwan and Zoph, Barret and Vaswani, Ashish and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={3286--3295},
  year={2019}
}

% Stand-Alone SA in vision models
@article{ramachandran2019stand,
  title={Stand-alone self-attention in vision models},
  author={Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jon},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

% Global SA
@article{shen2020global,
  title={Global self-attention networks for image recognition},
  author={Shen, Zhuoran and Bello, Irwan and Vemulapalli, Raviteja and Jia, Xuhui and Chen, Ching-Hui},
  journal={arXiv preprint arXiv:2010.03019},
  year={2020}
}

% ViT
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

% LeViT
@inproceedings{graham2021levit,
  title={Levit: a vision transformer in convnet's clothing for faster inference},
  author={Graham, Benjamin and El-Nouby, Alaaeldin and Touvron, Hugo and Stock, Pierre and Joulin, Armand and J{\'e}gou, Herv{\'e} and Douze, Matthijs},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={12259--12269},
  year={2021}
}

% Early Convs in Transformers
@article{xiao2021early,
  title={Early convolutions help transformers see better},
  author={Xiao, Tete and Singh, Mannat and Mintun, Eric and Darrell, Trevor and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={30392--30400},
  year={2021}
}

% Swin-T
@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

% SCOUTER
@inproceedings{li2021scouter,
  title={SCOUTER: Slot attention-based classifier for explainable image recognition},
  author={Li, Liangzhi and Wang, Bowen and Verma, Manisha and Nakashima, Yuta and Kawasaki, Ryo and Nagahara, Hajime},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1046--1055},
  year={2021}
}

$ Slot-Attention
@article{locatello2020object,
  title={Object-centric learning with slot attention},
  author={Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

% Conformer
@inproceedings{peng2021conformer,
  title={Conformer: Local features coupling global representations for visual recognition},
  author={Peng, Zhiliang and Huang, Wei and Gu, Shanzhi and Xie, Lingxi and Wang, Yaowei and Jiao, Jianbin and Ye, Qixiang},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={367--376},
  year={2021}
}

% Patchconvnet
@article{touvron2021augmenting,
  title={Augmenting convolutional networks with attention-based aggregation},
  author={Touvron, Hugo and Cord, Matthieu and El-Nouby, Alaaeldin and Bojanowski, Piotr and Joulin, Armand and Synnaeve, Gabriel and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:2112.13692},
  year={2021}
}

% Rethinking spatial in transformers
@inproceedings{heo2021rethinking,
  title={Rethinking spatial dimensions of vision transformers},
  author={Heo, Byeongho and Yun, Sangdoo and Han, Dongyoon and Chun, Sanghyuk and Choe, Junsuk and Oh, Seong Joon},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11936--11945},
  year={2021}
}

% FakeCAM + ADCC
@inproceedings{poppi2021revisiting,
  title={Revisiting the evaluation of class activation mapping for explainability: A novel metric and experimental analysis},
  author={Poppi, Samuele and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2299--2304},
  year={2021}
}

% GradCAM
@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={CVPR},
  year={2017}
}

% Network Inspection
@article{zhou2018interpreting,
  author={Zhou, Bolei and Bau, David and Oliva, Aude and Torralba, Antonio},
  journal={Trans. PAMI}, 
  title={Interpreting Deep Visual Representations via Network Dissection}, 
  year={2019},
  volume={41},
  number={9},
  pages={2131-2145},
 }

% CAM
@inproceedings{zhou2016learning,    
    author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
    title = {Learning Deep Features for Discriminative Localization},
    booktitle = {CVPR},
    year = {2016}
} 

%% Raw-Attention, Rollout Attention
@article{abnar2020quantifying,
  title={Quantifying attention flow in transformers},
  author={Abnar, Samira and Zuidema, Willem},
  journal={arXiv preprint arXiv:2005.00928},
  year={2020}
}

%% TIBAV
@inproceedings{chefer2021transformer,
  title={Transformer interpretability beyond attention visualization},
  author={Chefer, Hila and Gur, Shir and Wolf, Lior},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={782--791},
  year={2021}
}

% Fidelity 
@article{yeh2019fidelity,
  title={On the (in) fidelity and sensitivity of explanations},
  author={Yeh, Chih-Kuan and Hsieh, Cheng-Yu and Suggala, Arun and Inouye, David I and Ravikumar, Pradeep K},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

%% Review interp
@article{samek2021explaining,
  author={Samek, Wojciech and Montavon, Grégoire and Lapuschkin, Sebastian and Anders, Christopher J. and Müller, Klaus-Robert},
  journal={Proc. of the IEEE}, 
  title={Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications}, 
  year={2021},
  volume={109},
  number={3},
  pages={247-278},
 }

% Evaluation visualization
 @article{samek2016evaluating,
  title={Evaluating the visualization of what a deep neural network has learned},
  author={Samek, Wojciech and Binder, Alexander and Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and M{\"u}ller, Klaus-Robert},
  journal={IEEE transactions on neural networks and learning systems},
  volume={28},
  number={11},
  pages={2660--2673},
  year={2016},
  publisher={IEEE}
}

%% XGrad-CAM
@article{axiombased,
  title={Axiom-based grad-cam: Towards accurate visualization and explanation of cnns},
  author={Fu, Ruigang and Hu, Qingyong and Dong, Xiaohu and Guo, Yulan and Gao, Yinghui and Li, Biao},
  journal={arXiv preprint arXiv:2008.02312},
  year={2020}
}

%% Smooth Grad CAM
@article{smilkov2017smoothgrad,
  title={Smoothgrad: removing noise by adding noise},
  author={Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  journal={arXiv preprint arXiv:1706.03825},
  year={2017}
}

%% Attention is all you need
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

%% ConvNext
@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11976--11986},
  year={2022}
}

%% BERT
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


%% GAP
@article{lin2013network,
  title={Network in network},
  author={Lin, Min and Chen, Qiang and Yan, Shuicheng},
  journal={arXiv preprint arXiv:1312.4400},
  year={2013}
}

@article{mythos_interp,
  title={The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery.},
  author={Lipton, Zachary C},
  journal={Queue},
  volume={16},
  number={3},
  pages={31--57},
  year={2018},
  publisher={ACM New York, NY, USA}
}

%% Guided Backprop
@article{guidedbackprop,
  title={Striving for simplicity: The all convolutional net},
  author={Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1412.6806},
  year={2014}
}

%% Ablation-CAM
@INPROCEEDINGS{ablationcam,
  author={Desai, Saurabh and Ramaswamy, Harish G.},
  booktitle={2020 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Ablation-CAM: Visual Explanations for Deep Convolutional Network via Gradient-free Localization}, 
  year={2020},
  volume={},
  number={},
  pages={972-980},
  doi={10.1109/WACV45572.2020.9093360}}
  

%% Activation propagation #1
 @misc{shrikumar2017just,
      title={Not Just a Black Box: Learning Important Features Through Propagating Activation Differences}, 
      author={Avanti Shrikumar and Peyton Greenside and Anna Shcherbina and Anshul Kundaje},
      year={2017},
      eprint={1605.01713},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{shrikumar2019learning,
      title={Learning Important Features Through Propagating Activation Differences}, 
      author={Avanti Shrikumar and Peyton Greenside and Anshul Kundaje},
      year={2019},
      eprint={1704.02685},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

%% Noise perturbation
@misc{hsieh2021evaluations,
      title={Evaluations and Methods for Explanation through Robustness Analysis}, 
      author={Cheng-Yu Hsieh and Chih-Kuan Yeh and Xuanqing Liu and Pradeep Ravikumar and Seungyeon Kim and Sanjiv Kumar and Cho-Jui Hsieh},
      year={2021},
      eprint={2006.00442},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
%% More Baselines
@misc{haug2021baselines,
      title={On Baselines for Local Feature Attributions}, 
      author={Johannes Haug and Stefan Zürn and Peter El-Jiz and Gjergji Kasneci},
      year={2021},
      eprint={2101.00905},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{sturmfels2020visualizing,
  author = {Sturmfels, Pascal and Lundberg, Scott and Lee, Su-In},
  title = {Visualizing the Impact of Feature Attribution Baselines},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/attribution-baselines},
  doi = {10.23915/distill.00022}
}

@misc{kindermans2017unreliability,
      title={The (Un)reliability of saliency methods}, 
      author={Pieter-Jan Kindermans and Sara Hooker and Julius Adebayo and Maximilian Alber and Kristof T. Schütt and Sven Dähne and Dumitru Erhan and Been Kim},
      year={2017},
      eprint={1711.00867},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

%% Human Evolution, Pattern Recognition
@article{mattson2014superior,
  title={Superior pattern processing is the essence of the evolved human brain},
  author={Mattson, Mark P},
  journal={Frontiers in neuroscience},
  pages={265},
  year={2014},
  publisher={Frontiers}
}

%% 90% of Data transmitted is visual.
@article{potter2014detecting,
  title={Detecting meaning in RSVP at 13 ms per picture},
  author={Potter, Mary C and Wyble, Brad and Hagmann, Carl Erick and McCourt, Emily S},
  journal={Attention, Perception, \& Psychophysics},
  volume={76},
  pages={270--279},
  year={2014},
  publisher={Springer}
}

%% Greeks study on vision
@book{finger2001origins,
  title={Origins of neuroscience: a history of explorations into brain function},
  author={Finger, Stanley},
  year={2001},
  publisher={Oxford University Press}
}

%% Newton Locke Research Neuro
@article{swenson2010optics,
  title={Optics, Gender, and the Eighteenth-Century Gaze: Looking at Eliza Haywood's Anti-Pamela},
  author={Swenson, Rivka},
  journal={The Eighteenth Century},
  volume={51},
  number={1},
  pages={27--43},
  year={2010},
  publisher={University of Pennsylvania Press}
}

%% Leonardo in Vision
@article{visualleonardo,
author = {K. D. Keele},
title ={Leonardo Da Vinci on Vision},
journal = {Proceedings of the Royal Society of Medicine},
volume = {48},
number = {5},
pages = {384-390},
year = {1955},
doi = {10.1177/003591575504800512},
URL = {"\url{https://doi.org/10.1177/003591575504800512"}
},
eprint = {"\url{https://doi.org/10.1177/003591575504800512"}
}
}
