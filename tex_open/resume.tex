%--------------------------------------------------------------------------------------------------
\chapter*{Résumé}
\addcontentsline{toc}{chapter}{Résumé}
Les applications de vision par ordinateur utilisant les technologies de l'intelligence artificielle 
ont connu une évolution remarquable au cours de la dernière décennie. Les développements actuels en 
vision par ordinateur sont le résultat direct d'une meilleure utilisation du matériel, permettant 
la construction de modèles capables d'effectuer des tâches plus complexes au fil du temps. En ce 
qui concerne la reconnaissance d'images en particulier, les réseaux neuronaux convolutionnels et 
les transformateurs sont désormais capables d'identifier les images et leurs éléments, ainsi que 
d'attribuer une valeur sémantique à ces derniers, même dans des conditions difficiles. Ces modèles 
ont complètement changé le paysage de l'apprentissage profond et de la reconnaissance d'images, en 
pénétrant profondément dans la société avec l'automatisation de nombreuses tâches quotidiennes. 
Ainsi, il ne s'agit plus de savoir si un modèle peut effectuer cette tâche, mais plutôt de savoir 
comment ce modèle effectue cette tâche. Pour répondre à ces questions, un nouveau domaine de 
recherche a émergé : l'interprétabilité et l'intelligence artificielle explicative.\\

\noindent Dans cette thèse, notre objectif est de comprendre et de développer des modèles 
d'interprétabilité pour les modèles de reconnaissance d'images de pointe. Nous présentons et 
expliquons brièvement certains des modèles de reconnaissance d'images les plus performants et 
pertinents pour les Réseaux de Neurones Convolutifs et les Transformers. Ensuite, nous examinons 
les approches actuelles en matière d' interpr\'etabilit\'e conçues pour fournir des explications, ainsi 
que leurs protocoles d' évaluation. Nous faisons des observations sur ces méthodes et protocoles 
d'évaluation, mettant en évidence les difficultés rencontrées et suggérant des idées pour surmonter 
leurs limitations.\\

\noindent En particulier, une nouvelle méthode d'attribution est proposée, garantissant que les 
régions mises en évidence dans une image maximisent une probabilité de classe donnée. À partir de 
cette méthode, nous observons que les informations les plus importantes corrélées à une prédiction 
de classe ne se trouvent pas seulement dans l'objet d'intérêt. Au contraire, le contexte transmet 
également des informations saillantes : elles sont généralement réparties dans toute l'image. De 
plus, une addition aux architectures existantes est introduite, cette méthode améliore la confiance 
prédictive, renforçant ainsi les propriétés d'interprétabilité des modèles de reconnaissance 
d'images performants. Enfin, nous proposons un paradigme d'apprentissage qui débruite les gradients 
dans l'espace d'image, améliorant ainsi les propriétés d'interprétabilité. Cette proposition finale 
démontre des améliorations 
sur les métriques d'interprétabilité pour de petits ensembles de données et modèles, tout en 
montrant des promesses pour son passage à l'échelle sur de plus grands ensembles de données et 
architectures.\\

\vspace{0.5cm}
%Keywords: deep learning, image processing, attributions, interpretability
Mots clés: Apprentissage Profond, reconaissance d'image, interpretabilité, explicabilité.
