## To Do

**Introduction**
- [x] Your very first paragraph places your work in a very large context! It's nice ;) Are you sure most of the information humans process comes from vision?
-[x] "Computer vision aims to replicate human capabilities with a machine." -> specify visual capabilities
-[ ]End of first paragraph page 18: add few references and a word to say these works and more are described with more details in chapter 1
-[ ]The last 5 lines of 2nd paragraph page 18 are not clear.. reformulate?
-[x] p18-19 "These objectives"
-[x]These objectives -> Thesis objectives?
-[x]modes -> models
-[x]"existing approaches may these properties » ??
-[ ]The first paragraph highlights existing limitations and issues which is good. The rest is I think too verbose and does not well introduce your objectives. You should be more concise, don’t hesitate to use bold font to make clear your objectives..
-[x]Page 19 mostly discuss on particular XAI issues that make your thesis motivation. Try to highlight them a bit more and connect it with your dissertation outline. I think as motivation you should also mention RGPD and AI Act that push towards more XAI and restrict some applications of CV/AI..
p20 Ok

**Background**


-[ ] p21 you should also mention neurosciences!
-[x] advent -> advances ?
-[x] comptational -> computational 
-[x] p22 maybe cite ResNet as a major model still SOTA even if developed later
-[ ] p23 the part from "With the adoption of deep image recognition into society" until end of the page should move to introduction, IMO while mentioning AI Act 
-[x] p25-26 could be extended with more details
-[x] add convolution formulas and elaborate on its connection with previous HOG and SIFT..
-[x] add pooling purpose and formulas (and its gradients?) (done fig instead)
-[ ]add AlexNet figure! As well as ReLu as a key ingredient they also relied on Dropout that helped a lot for better convergence and generalization.
-[ ] p26-28 lack of structure.. Need more figures and formulas. Discuss more skip connections and its gradients effect
-[ ] you could mention the links between ODEs and ResNet which is interesting to understand why it works well http://proceedings.mlr.press/v119/dong20c/dong20c.pdf
-[x] p28 the paragraph on optimization improvement from SGD to Adam is not directly related to CNNs (and out of the scope of your work)
-[x] p30 you mention receptive field but never defined (should be in CNN section)
-[ ] p34 in addition to the discussion you should end the section 1.4 with a big table to summarize all the models you have reviewed
-[ ] P35-46 on interpretability is more well structured but very dense. Maybe add a figure or table to illustrate and give an overview of methods
-[x] p42 the separation with "CAM-based methods" as a post hoc method and "CAM-based saliency maps" in another section is weird.. Maybe put Attention-based before CAM-based and specify you develop CAM methods in the next section. 
-[ ] p42 last paragraph should also elaborate on hierarchical representations in CNNs from low level pattern detectors to higher semantics and task specifics patterns
-[x] p42 "Consider a classifier network : f x" -> "Consider a classifier network f :" 
-[ ] p46 a table to summarize reviewed interpretability methods would be great 
-[ ] P46-51 on evaluation is ok but dense. You should also add figures to illustrate.. I guess possible for INS and DEL. The blue for references is strange.. 
-[ ] p50 "Interpretable object localization" could end up with a discussion on the differences of human-centric interpretability vs model-centric interpretability as in introduction 
-[ ] p51-54 the discussion must also position your work from existing methods you have reviewed. Here you should add links to the next chapters that address some issues you are discussing in this section.


